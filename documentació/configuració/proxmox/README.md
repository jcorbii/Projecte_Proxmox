# 3. ğŸ–¥ï¸ ImplementaciÃ³ del ClÃºster Proxmox

A continuaciÃ³ et detallem pas a pas com crear un clÃºster en Proxmox i unir-hi altres nodes.

---

## ğŸ› ï¸ 1. Crear el ClÃºster

1. Accedeix a un dels nodes de Proxmox.
2. Ves a **Datacenter â†’ Cluster** des del menÃº lateral esquerre.
3. Fes clic a **Crear ClÃºster** (`Create Cluster`).

![Pantalla inicial del cluster](../../../img/image-56.png)



4. Ompli les dades del clÃºster:

   * **Nom del ClÃºster**
   * **InterfÃ­cie de xarxa**
   * Altres parÃ metres segons la teua configuraciÃ³

<p align="center">
  <img src="../../../img/image-57.png" alt="Crear cluster pas 1" />
</p>

<p align="center">
  <img src="../../../img/image-58.png" alt="Crear cluster pas 2" />
</p>

5. Un cop creat, veurÃ s el node com a part del clÃºster.

![Cluster creat](../../../img/image-59.png)

---

## ğŸ”— 2. Unir Nodes al ClÃºster

Per afegir un altre node al clÃºster:

1. Accedeix al segon node i ves a **Datacenter â†’ Cluster**.
2. Fes clic a **Unir-se al clÃºster** (`Join Cluster`).

![Unir-se al cluster](../../../img/image-60.png)

3. A continuaciÃ³, haurÃ s dâ€™introduir la **informaciÃ³ del clÃºster**.

<p align="center">
  <img src="../../../img/image-61.png" alt="Formulari unir-se" />
</p>

4. Per obtindre aquesta informaciÃ³, torna al node principal del clÃºster i fes clic a **Join Information**.

![InformaciÃ³ per unir-se](../../../img/image-62.png)

5. Copia aquesta informaciÃ³ i torna al node secundari. Enganxa-la al formulari per unir-se.

<p align="center">
  <img src="../../../img/image-63.png" alt="Enganxar informaciÃ³" />
</p>

6. Fes clic a **Unir-se**. Si tot Ã©s correcte, el node sâ€™afegirÃ  automÃ ticament al clÃºster.

![Node afegit](../../../img/image-64.png)

---

## â• 3. Afegir mÃ©s nodes

Per afegir mÃ©s nodes, repeteix exactament el mateix procÃ©s:

* Accedeix al node
* Ves a **Datacenter â†’ Cluster**
* Fes clic a **Unir-se al clÃºster**
* Copia la informaciÃ³ del node principal
* Enganxa-la i uneix el node

---

ğŸ”š I amb aixÃ² ja tindrÃ s un clÃºster Proxmox funcional amb diversos nodes!

![ComprovaciÃ³](../../../img/image-65.png)

Perfecte! Comencem pel punt **4.1 IntroducciÃ³ a Ceph i integraciÃ³ amb Proxmox**. Et deixe a continuaciÃ³ una proposta redactada en valenciÃ  formal, clara i adequada per al teu projecte:

---

### ğŸ§  4 IntroducciÃ³ a **Ceph** i IntegraciÃ³ amb **Proxmox VE**

**Ceph** Ã©s una plataforma dâ€™emmagatzematge distribuÃ¯t de codi obert dissenyada per oferir alta disponibilitat, escalabilitat i rendiment, sense punts Ãºnics de fallada. El seu funcionament es basa en tres components principals:

* **OSD (Object Storage Daemon):** Gestiona el disc dur on sâ€™emmagatzema la informaciÃ³.
* **MON (Monitor):** Controla lâ€™estat del clÃºster, mantÃ© el mapa del clÃºster i garanteix el consens entre nodes.
* **MGR (Manager):** Proporciona funcionalitats addicionals de monitoratge i interfÃ­cie web.

Ceph permet oferir emmagatzematge per a:

* MÃ quines virtuals (amb RBD â€“ Rados Block Device)
* Sistemes dâ€™arxius (CephFS)
* Objectes (compatible amb S3)

---

#### ğŸ”— IntegraciÃ³ amb Proxmox VE

**Proxmox VE** incorpora suport nadiu per a Ceph, cosa que facilita la seua instalÂ·laciÃ³, gestiÃ³ i integraciÃ³ des de la mateixa interfÃ­cie web o via lÃ­nia de comandes.

GrÃ cies a aquesta integraciÃ³:

* Es pot configurar Ceph directament des de la interfÃ­cie de **Datacenter â†’ Ceph**
* Els discos Ceph (RBD) poden ser utilitzats com a **emmagatzematge de mÃ quines virtuals** i **contenidors (LXC)**
* El sistema garanteix **alta disponibilitat**, ja que les dades estan replicades en diversos nodes
* Permet una **escala horitzontal** fÃ cil, afegint mÃ©s discos o nodes al clÃºster Ceph

---

ğŸ’¡ **Per quÃ¨ utilitzar Ceph en Proxmox?**

* Elimina la dependÃ¨ncia de sistemes dâ€™emmagatzematge extern (NFS, iSCSI, etc.)
* Millora la tolerÃ ncia a fallades i la continuÃ¯tat del servei
* Ofereix una gestiÃ³ centralitzada i unificada del clÃºster i lâ€™emmagatzematge

Perfecte! AcÃ­ tens el punt **4.2 InstalÂ·laciÃ³ i configuraciÃ³ de Ceph al clÃºster**, redactat en valenciÃ  formal i pensat per a un projecte tÃ¨cnic:

---

### 4.2 âš™ï¸ InstalÂ·laciÃ³ i ConfiguraciÃ³ de **Ceph** al ClÃºster

La instalÂ·laciÃ³ de Ceph en un entorn **Proxmox VE** es pot fer de manera centralitzada i senzilla grÃ cies a la seua integraciÃ³ nativa. A continuaciÃ³ es detallen els passos principals per a desplegar Ceph en un clÃºster de Proxmox:

---

#### ğŸ§© Requisits previs

Abans de comenÃ§ar amb la instalÂ·laciÃ³, cal assegurar:

* Tots els nodes tenen lâ€™hora sincronitzada via **NTP**
* Una xarxa especÃ­fica o VLAN dedicada per al trÃ nsit de Ceph (preferiblement amb baixa latÃ¨ncia i alta amplada de banda)
* Discos dedicats per a Ceph (no utilitzar el mateix disc que el sistema operatiu)
* Una configuraciÃ³ bÃ sica del clÃºster de Proxmox ja establida

---

#### ğŸ› ï¸ Passos dâ€™instalÂ·laciÃ³

1. **Accedir a la interfÃ­cie web de Proxmox**

   * Ves a `Datacenter â†’ Ceph`

2. **InstalÂ·lar Ceph als nodes**

   * Selecciona cada node on vols desplegar Ceph
   * A lâ€™apartat `Ceph`, fes clic a **Install Ceph**
   * El sistema instalÂ·larÃ  automÃ ticament els paquets necessaris (`ceph`, `ceph-common`, etc.)

![alt text](../../../img/image-66.png)

![alt text](../../../img/image-67.png)

3. **Crear els monitors (MON)**

   * Un mÃ­nim de **tres monitors** Ã©s recomanat per garantir el quorum
   * Des de lâ€™apartat `Monitor`, fes clic a **Create Monitor**

![alt text](../../../img/image-68.png)

![alt text](../../../img/image-69.png)

4. **Afegir el gestor (MGR)**

   * Necessari per a la interfÃ­cie grÃ fica i gestiÃ³ avanÃ§ada
   * Creaâ€™l des de la mateixa pestanya amb el botÃ³ **Create Manager**

![alt text](../../../img/image-70.png)

5. **Afegir els OSDs (Object Storage Daemons)**

   * Els OSDs sÃ³n els processos que gestionen els discos durs del clÃºster
   * Ves a `OSD â†’ Create OSD`, selecciona el disc fÃ­sic i creaâ€™l
   * Repeteix el procÃ©s per a cada node i disc dedicat

![alt text](../../../img/image-71.png)


<p align="center">
  <img src="../../../img/image-72.png" alt="Enganxar informaciÃ³" />
</p>

* Com tenim 2 discos per cada node (menos en el node 3 que sols hi ha 1)de proxmox haurem de repetir el proccess dos voltes

**Node 1:**

![alt text](../../../img/image-73.png)

**Node 2**

![alt text](../../../img/image-74.png)

**Node 3**

![alt text](../../../img/image-75.png)

1. **(Opcional) Crear un MDS (Metadata Server)**

   * NomÃ©s si vols utilitzar **CephFS** com a sistema de fitxers compartit

    ##### **ğŸ“‚ QuÃ¨ sÃ³n els metadades?**
    Els metadades sÃ³n informaciÃ³ sobre els fitxers, com ara:

    Noms de fitxers i directoris

    Jerarquia de carpetes

    Permisos dâ€™accÃ©s

    Propietaris

    Dates de creaciÃ³ o modificaciÃ³

    
    ##### ğŸ§  QuÃ¨ fa exactament el MDS?
    
    Quan utilitzes CephFS (el sistema de fitxers distribuÃ¯t de Ceph), el Metadata Server:

    Controla lâ€™estructura i organitzaciÃ³ del sistema de fitxers

    Processa operacions com ls, mkdir, rm, mv, etc.

    Fa que les consultes de fitxers siguen rÃ pides i escalables

    Allibera als OSDs dâ€™aquesta tasca perquÃ¨ es centren nomÃ©s en llegir i escriure dades

Perfecte! AcÃ­ tens el punt **4.3 CreaciÃ³ de pools dâ€™emmagatzematge** redactat de manera formal i clara en valenciÃ , seguint lâ€™estil dels punts anteriors:

---

### 4.3 ğŸ—ï¸ CreaciÃ³ de Pools dâ€™Emmagatzematge en Ceph

Els **pools dâ€™emmagatzematge** sÃ³n una part fonamental en lâ€™arquitectura de **Ceph**, ja que representen els espais lÃ²gics on es distribueixen les dades entre els diferents OSDs del clÃºster. A cada pool se li pot assignar una funciÃ³ especÃ­fica, com ara allotjar mÃ quines virtuals, contenidors o fitxers de CephFS.

---

### ğŸ” QuÃ¨ Ã©s un Pool?

Un **pool** Ã©s una agrupaciÃ³ lÃ²gica dâ€™objectes dins del clÃºster Ceph. Cada objecte dins dâ€™un pool es reparteix entre els OSDs segons una polÃ­tica de distribuciÃ³ definida, garantint aixÃ­ la replicaciÃ³ i la tolerÃ ncia a fallades.

---

### ğŸ› ï¸ CreaciÃ³ dâ€™un Pool pas a pas en Proxmox VE

1. Accedeix a la interfÃ­cie web de **Proxmox VE**
2. Ves a `Datacenter â†’ Ceph â†’ Pools`
3. Fes clic a **Create**

![alt text](../../../img/image-76.png)

4. Emplena els camps segÃ¼ents:



   * **Nom del pool:** (ex. `vm_data`, `cephfs_data`, `backups`)
   * **Nombre de rÃ¨pliques (Size):** recomanat mÃ­nim **3** per a alta disponibilitat
   * **Min. rÃ¨pliques (Min. Size):** mÃ­nim **2** per a mantenir el servei actiu amb una fallada
   * **Crush Rule:** regla de distribuciÃ³ entre els dispositius de disc

<p align="center">
  <img src="image-77.png" alt="CreaciÃ³ de pool en Proxmox" />
</p>

1. Fes clic a **Create** i espera a que el pool aparega a la llista

![alt text](../../../img/image-78.png)

Al pas d'un temps podem veure com en els nodes apareix l'almacenament del ceph.

![alt text](../../../img/image-79.png)

---

### ğŸ§  Consideracions importants

* Els pools amb mÃ©s rÃ¨pliques consumeixen mÃ©s espai perÃ² ofereixen mÃ©s redundÃ ncia.
* Ã‰s possible crear **pools separats** per a diferents usos (ex: un per a VM i un altre per a CephFS).
* Es poden utilitzar **regles CRUSH** per controlar com es distribueixen les dades per racks, discos o ubicacions fÃ­siques.

---

### âœ… Resultat

Amb el pool creat, ja pots:

* Assignar-lo com a **emmagatzematge de mÃ quines virtuals (RBD)**
* Utilitzar-lo per a **CephFS**
* Monitorar el seu estat i Ãºs des de la pestanya de **Ceph â†’ Pools**

Perfecte! A continuaciÃ³ et presente el punt **4.4 Proves de rendiment i replicaciÃ³** redactat de manera formal i clara, mantenint lâ€™estil del teu projecte en valenciÃ :

---

### 4.4 ğŸš€ Proves de Rendiment i ReplicaciÃ³ en Ceph

Una vegada el clÃºster Ceph estÃ  desplegat i operatiu, Ã©s fonamental realitzar proves de **rendiment** i **replicaciÃ³** per a verificar el correcte funcionament de lâ€™emmagatzematge distribuÃ¯t, aixÃ­ com garantir la **fiabilitat** i **eficiÃ¨ncia** del sistema.

---

### ğŸ“Š Proves de rendiment

Les proves de rendiment ens permeten mesurar la **velocitat de lectura i escriptura** dels dispositius Ceph, aixÃ­ com la **latÃ¨ncia** i **capacitat de resposta** del clÃºster.

#### ğŸ§ª Eines recomanades:

* `rados bench` â†’ eina prÃ²pia de Ceph per mesurar el rendiment de lectura/escriptura
* `fio` â†’ eina externa per fer proves personalitzades dâ€™I/O

#### Exemple amb `rados bench`:

```bash
# Prova d'escriptura durant 60 segons
rados bench -p vm-data 60 write --no-cleanup

# Prova de lectura seqÃ¼encial
rados bench -p vm-data 60 seq

# Prova de lectura aleatÃ²ria
rados bench -p vm-data 60 rand
```

<p align="center">
  <img src="../../../img/image-80.png" alt="CreaciÃ³ de pool en Proxmox" />
</p>

### âœ… Resultat esperat

* Les proves dâ€™escriptura i lectura han de mostrar valors de rendiment estables i adequats segons el teu hardware.
* La replicaciÃ³ ha de funcionar de manera automÃ tica, garantint la integritat i disponibilitat de les dades davant qualsevol fallada.

#### ğŸ“Œ Exemple de comandes via CLI

Alternativament, es pot fer la instalÂ·laciÃ³ via lÃ­nia de comandes:

```bash
# InstalÂ·lar Ceph
pveceph install

# Crear un monitor
pveceph mon create

# Crear un OSD
pveceph osd create /dev/sdX
```

---

### âœ… Resultat

Una vegada configurats els **MON**, **MGR** i **OSD**, el clÃºster Ceph estarÃ  operatiu. Ja pots procedir a:

* Crear **pools dâ€™emmagatzematge**
* Assignar-los com a backend per a mÃ quines virtuals
* Monitorar lâ€™estat del clÃºster des de la interfÃ­cie de Proxmox

---

### â™»ï¸ Proves de replicaciÃ³

Ceph replica les dades entre OSDs segons la configuraciÃ³ de rÃ¨pliques (per defecte 3). Ã‰s important verificar que:

1. **Les dades es repliquen correctament** a mÃºltiples discos.
2. **Quan cau un OSD o node**, Ceph automÃ ticament redistribueix les rÃ¨pliques.

#### ğŸ”„ Prova de fallada simulada:

1. Apaga un OSD manualment:

   ```bash
   systemctl stop ceph-osd@X
   ```

   (on `X` Ã©s el nÃºmero de lâ€™OSD)


2. Observa com Ceph reporta lâ€™estat *degraded* i com reubica les dades.

![Observar Ceph](../../../img/image-81.png)

3. Torna a engegar lâ€™OSD i comprova la **reestructuraciÃ³ automÃ tica**:

   ```bash
   systemctl start ceph-osd@X
   ```

![RestauraciÃ³](../../../img/image-82.png)

---

### ğŸ“ˆ 4.5 GestiÃ³ i Monitoratge de **Ceph**

Una vegada desplegat el clÃºster **Ceph**, Ã©s fonamental realitzar una gestiÃ³ i monitoratge continuat per garantir lâ€™estabilitat, el rendiment i la disponibilitat de les dades. Proxmox VE ofereix una **integraciÃ³ nativa amb Ceph**, que facilita tant el control operatiu com la detecciÃ³ anticipada de possibles incidÃ¨ncies.

---

### ğŸ› ï¸ Eines de gestiÃ³ disponibles

Proxmox proporciona diversos mÃ¨todes per gestionar Ceph:

#### ğŸ“Œ InterfÃ­cie web de Proxmox VE

Des de la GUI es pot accedir a:

* **Estat del clÃºster**: `Datacenter â†’ Ceph â†’ Status`
* **GestiÃ³ dâ€™OSDs**: afegir, eliminar o veure lâ€™estat dels Object Storage Daemons
* **Monitors (MONs)** i **Managers (MGRs)**: estat, creaciÃ³ i falles
* **CreaciÃ³ i eliminaciÃ³ de pools**
* **ConfiguraciÃ³ i gestiÃ³ de CephFS**

#### ğŸ”§ LÃ­nia de comandes (CLI)

Per a gestiÃ³ avanÃ§ada i automatitzacions:

```bash
ceph status             # Estat general del clÃºster
ceph osd tree           # VisualitzaciÃ³ jerÃ rquica dels OSDs
ceph df                 # Ãšs dâ€™espai en pools
ceph health detail      # InformaciÃ³ detallada de salut
ceph osd out/in <id>    # Marcar un OSD com a fora o dins del clÃºster
```

---

### ğŸ“Š Monitoratge actiu del clÃºster

Els principals parÃ metres a controlar de forma contÃ­nua sÃ³n:

* **Salut del clÃºster (HEALTH\_OK / WARN / ERR)**
* **Nombre dâ€™OSDs actius i en lÃ­nia**
* **Estat dels MON i MGR**
* **Ãšs de lâ€™espai per pool i per OSD**
* **LatÃ¨ncies de lectura i escriptura**
* **Rebalanceig de dades en cas de fallada o afegit de nous discos**

---

### ğŸ“¡ IntegraciÃ³ amb eines externes

Encara que Proxmox proporciona visualitzaciÃ³ bÃ sica, pots integrar Ceph amb eines de monitoratge mÃ©s potents com:

* **Prometheus + Grafana** (via Ceph MGR modules)
* **Zabbix** o **Nagios**, mitjanÃ§ant plugins
* Alertes per correu electrÃ²nic o sistemes de notificaciÃ³

---

### ğŸ’¡ Recomanacions de manteniment

* ğŸ” **Revisar lâ€™estat del clÃºster regularment**
* ğŸ§ª **Simular fallades controlades** per validar la replicaciÃ³ i recuperaciÃ³
* ğŸ“¦ **No sobrecarregar els OSDs**; mantindre un marge de capacitat lliure
* ğŸ›‘ **Evitar la pÃ¨rdua simultÃ nia de mÃºltiples discos** amb rÃ¨pliques mÃ­nimes

---

### âœ… Resultat

Amb una correcta gestiÃ³ i monitoratge, es garanteix que el clÃºster Ceph oferisca un rendiment estable, altament disponible i resistent a fallades, adaptat a les necessitats de lâ€™entorn virtualitzat en Proxmox VE.


Perfecte! AcÃ­ tens el punt **5. Alta Disponibilitat (HA)** completament redactat en valenciÃ  formal i tÃ¨cnic, amb icones i una estructura clara per a incloureâ€™l al teu projecte:

---

## ğŸ›¡ï¸ 5. Alta Disponibilitat (**HA**) en Proxmox VE

La **Alta Disponibilitat (High Availability)** Ã©s una funcionalitat clau en entorns crÃ­tics, ja que permet garantir la **continuÃ¯tat del servei** davant la caiguda dâ€™un node. Amb Proxmox VE, aquesta caracterÃ­stica sâ€™integra de manera nativa quan es treballa amb un **clÃºster**.

---

### âš™ï¸ 5.1 ActivaciÃ³ del Gestor HA en Proxmox

Per a fer Ãºs de la funcionalitat HA, cal que:

1. El clÃºster estiga format per almenys **3 nodes** (per mantenir el **quorum**)
2. Els nodes tinguen **Corosync** i **pve-ha-crm** actius
3. Els recursos (VM o CT) estiguen ubicats en **storage compartit** o Ceph

#### ğŸ”„ Procediment:

* Ves a `Datacenter â†’ HA`
* Asseguraâ€™t que el **HA Manager** estÃ  actiu
* Cada node mostrarÃ  el seu estat (online, standby, etc.)

<p align="center">
  <img src="../../../img/image-83.png" alt="Gestor HA" />
</p>

---

### ğŸ§© 5.2 DefiniciÃ³ de Grups HA

Els **grups HA** permeten organitzar i assignar mÃ quines virtuals o contenidors per a gestionar millor les polÃ­tiques de tolerÃ ncia a fallades.

#### CreaciÃ³ dâ€™un grup HA:

1. Ves a `Datacenter â†’ HA â†’ Groups`

![alt text](../../../img/image-84.png)

1. Fes clic a **Create**
2. Assigna:

<p align="center">
  <img src="../../../img/image-85.png" alt="Gestor HA" />
</p>

   * **Nom del grup**
   * **Nodes preferits** per executar el servei
   * **Prioritats** (per decidir on sâ€™hauria de migrar en cas de fallada)

DesprÃ©s, quan crees o edites una VM/CT, pots assignar-la a un grup HA.

---

### ğŸ” 5.3 Proves de TolerÃ ncia a Fallades (Failover de MÃ quines Virtuals)

Per assegurar el correcte funcionament de la configuraciÃ³ HA, Ã©s recomanable fer proves de **failover controlades**:

#### Escenari de prova:

1. Assigna una VM a un grup HA

<p align="center">
  <img src="../../../img/image-86.png" alt="Gestor HA" />
</p>

2. Para o apaga un node manualment

![alt text](../../../img/image-87.png)

3. Observa com la VM Ã©s **migrada automÃ ticament** a un altre node disponible
4. Verifica que el servei continua operatiu sense intervenciÃ³ manual

![alt text](../../../img/image-89.png)

ğŸ” Es pot monitorar aquest procÃ©s des de `Datacenter â†’ HA â†’ Status`.

![alt text](../../../img/image-90.png)

Per descomptat! AcÃ­ tens el fragment redactat de manera formal i clara, ideal per afegir com a continuaciÃ³ dins del punt 5.4 o com un subapartat prÃ ctic de **recuperaciÃ³ post-fallada**:

---

### ğŸ’¡ 5.4 Casos dâ€™Ãšs i RecuperaciÃ³ davant Caigudes de Nodes

Els entorns amb HA actiu poden recuperar-se de forma automÃ tica en diferents situacions:

* ğŸ”Œ **Fallada de hardware o energia** en un node
* ğŸ§¯ **Actualitzacions crÃ­tiques** que requereixen reinici
* âš™ï¸ **Errors de sistema** o problemes de rendiment greu

En cada cas:

* El sistema HA detecta la fallada
* MigraciÃ³ automÃ tica a nodes disponibles
* RestauraciÃ³ del servei amb **temps mÃ­nim dâ€™interrupciÃ³**

#### Exemple prÃ ctic:

Una mÃ quina virtual crÃ­tica (servidor web, base de dades, etc.) estÃ  configurada amb HA. Si el node cau inesperadament, aquesta VM es reinicia en un altre node en qÃ¼estiÃ³ de segons, garantint la continuÃ¯tat del servei.

---

### âœ… Resultat

Amb la configuraciÃ³ HA en Proxmox VE, es millora significativament la **resiliÃ¨ncia de la infraestructura virtualitzada**, assegurant que els serveis essencials estiguen disponibles **24/7**, fins i tot davant fallades greus.

---


### ğŸ” RecuperaciÃ³ manual de mÃ quines HA al seu node original

DesprÃ©s dâ€™una **caiguda temporal dâ€™un node** del clÃºster, el sistema **HA de Proxmox** trasllada automÃ ticament les mÃ quines virtuals o contenidors afectats a un altre node disponible per garantir la continuÃ¯tat del servei.

Un cop el node original torna a estar **en lÃ­nia i estable**, Ã©s **recomanable migrar manualment** les mÃ quines al seu node d'origen per:

* Recuperar lâ€™equilibri de cÃ rrega del clÃºster
* Retornar els recursos als seus entorns habituals
* Preparar el sistema per a futures fallades

---

### âš™ï¸ Procediment per a migrar una mÃ quina HA al node original

1. Accedeix a la interfÃ­cie web de Proxmox
2. Ves al node on actualment estÃ  executant-se la mÃ quina
3. Selecciona la mÃ quina virtual o contenidor
4. Fes clic a **"Migrate"**
5. Tria com a destinaciÃ³ el **node original** (ex: `node3`)
6. Confirma lâ€™operaciÃ³

ğŸ“Œ *Nota:* La migraciÃ³ es pot fer en calent (**live migration**) si la mÃ quina suporta aquesta funcionalitat (generalment les VMs amb discs en Ceph o ZFS compartit).

---

### âœ… Resultat

Amb aquest procÃ©s, la mÃ quina recupera la seua ubicaciÃ³ inicial, mantenint-se dins del grup HA i **preparada per a futures gestions automÃ tiques** de tolerÃ ncia a fallades.

![alt text](../../../img/image-91.png)

---

## ğŸ‘¥ 7. GestiÃ³ dâ€™Usuaris i Pools de Recursos

En entorns virtualitzats compartits, com un clÃºster de **Proxmox VE**, Ã©s fonamental establir una **gestiÃ³ dâ€™usuaris estructurada**, amb **permisos diferenciats** i assignaciÃ³ clara de **recursos**, per garantir la **seguretat, control i eficiÃ¨ncia operativa**.

---

### ğŸ” 7.1 CreaciÃ³ de Rols Personalitzats i Permisos

**Proxmox VE** ofereix un sistema de permisos basat en rols, que permet definir quÃ¨ pot fer cada usuari dins del sistema. Aquest model RBAC (Role-Based Access Control) es basa en tres elements:

* **Usuaris** (local, LDAP o via PAM)
* **Rols** (conjunts de permisos)
* **Objectes** (nodes, VM, storage, etc.)

#### ğŸ”§ CreaciÃ³ dâ€™un rol personalitzat:

1. Ves a `Datacenter â†’ Permissions â†’ Roles`
2. Fes clic a **Add**
3. Assigna un nom (ex. `gestor_vm`)
4. Selecciona els permisos especÃ­fics:

   * `VM.Allocate`
   * `VM.Config.Disk`
   * `VM.Console`
   * `Sys.Console`

![alt text](../../../img/image-92.png)

![alt text](../../../img/image-93.png)

#### â• AssignaciÃ³ del rol:

1. Ves a `Permissions â†’ Add â†’ Users`
2. Selecciona:

   * **Path:** Ã rea de control (`/`, `/vms`, `/pool/nom`, etc.)
   * **User:** usuari o grup
   * **Role:** el rol que has creat

AixÃ² permet donar accÃ©s restringit a determinats recursos dins del clÃºster.

![alt text](../../../img/image-94.png)

![alt text](../../../img/image-95.png)

En este cas he creat un usuari de prova per a assignar el rol creat.

![alt text](../../../img/image-96.png)

---

### ğŸ—‚ï¸ 7.2 DefiniciÃ³ de Pools de Recursos

Els **pools** sÃ³n agrupacions lÃ²giques de recursos (VMs, CTs, discos, etc.) que permeten facilitar la gestiÃ³, especialment en entorns multiusuari o amb departaments diferenciats.

#### ğŸ› ï¸ CreaciÃ³ dâ€™un pool:

1. Ves a `Datacenter â†’ Permissions â†’ Pools`
2. Fes clic a **Create**

![alt text](../../../img/image-97.png)

1. Emplena:

   * **Nom del pool:** ex. `departament_it`, `desenvolupament`
   * **DescripciÃ³** (opcional)

![alt text](../../../img/image-98.png)

1. Afegeix les VMs o CTs desitjades al pool

En este cas anem a fer que el usuari proba puga vore la vm 108(Windows10)

![alt text](../../../img/image-99.png)

Assignacio del pool al usuari proba amb el rol  que hem creat.

![alt text](../../../img/image-100.png)

Els pools sÃ³n Ãºtils per:

* Aplicar permisos a grups dâ€™usuari de forma mÃ©s eficient
* Organitzar recursos segons projectes o Ã rees de treball
* Limitar lâ€™accÃ©s nomÃ©s a les mÃ quines assignades

---

### ğŸ‘¤ 7.3 GestiÃ³ Delegada i Multiusuari

Amb els **rols** i **pools**, es pot habilitar un entorn **multiusuari segur**, on cada usuari o equip tinga accÃ©s nomÃ©s als recursos que li pertoquen.

#### Exemple de gestiÃ³ delegada:

* **Usuari:** `anna@pve`
* **Pool assignat:** `marketing_vms`
* **Rol aplicat:** `PVEVMUser` (amb permisos per iniciar/parar/migrar mÃ quines)
* Resultat: Anna nomÃ©s pot gestionar les VMs del pool `marketing_vms`, sense accedir a cap altre recurs del sistema

![alt text](../../../img/image-101.png)

![alt text](../../../img/image-102.png)

![alt text](../../../img/image-103.png)

![alt text](../../../img/image-104.png)

---

### âœ… Beneficis

* ğŸ”’ Major seguretat mitjanÃ§ant la separaciÃ³ de privilegis
* ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Facilitat per delegar la gestiÃ³ a equips tÃ¨cnics o usuaris finals
* ğŸ§© Escalabilitat per a entorns educatius, empresarials o d'hosting

---

## ğŸ§ª Casos PrÃ ctics de GestiÃ³ Delegada i Multiusuari en Proxmox VE

### ğŸ“ **Cas 1: Entorn educatiu amb alumnes de prÃ ctiques**

#### Escenari:

Lâ€™institut ha desplegat un clÃºster de Proxmox per a alumnes del cicle de sistemes. Cada alumne ha de gestionar una VM prÃ²pia, perÃ² sense accÃ©s al sistema complet.

#### ConfiguraciÃ³:

* **Usuari:** `alumne01@pve`
* **Pool:** `alumnes`
* **VM assignada:** `vm105` (Debian prÃ ctica)
* **Rol:** `PVEVMUser`

#### Resultat:

Lâ€™alumne pot:

* Engegar/parar la seua VM
* Accedir per consola
* No pot crear ni esborrar mÃ quines
* No pot veure cap altra VM

---

### ğŸ¢ **Cas 2: Departament de Desenvolupament en una empresa**

#### Escenari:

Lâ€™equip de desenvolupament necessita accedir a diverses mÃ quines de testing, perÃ² no ha de poder modificar la infraestructura general.

#### ConfiguraciÃ³:

* **Usuaris:** `david@pve`, `jordi@pve`
* **Pool:** `dev_pool`
* **Rols:** `gestor_vm_custom` (creat amb permisos limitats com `VM.Console`, `VM.Start`, `VM.Shutdown`)

#### Resultat:

Els usuaris poden:

* Utilitzar i gestionar les seues VMs
* No poden crear VMs noves ni modificar configuracions globals

---

### ğŸ› ï¸ **Cas 3: TÃ¨cnic amb accÃ©s complet a un node concret**

#### Escenari:

Un tÃ¨cnic extern colÂ·labora en la gestiÃ³ de sistemes, perÃ² nomÃ©s se li vol donar accÃ©s al node `node3`.

#### ConfiguraciÃ³:

* **Usuari:** `tecnic@pve`
* **Ã€rea assignada:** `/nodes/node3`
* **Rol:** `PVEAdmin`

#### Resultat:

TÃ© accÃ©s complet nomÃ©s a les mÃ quines i configuraciÃ³ dâ€™eixe node, perÃ² no pot accedir a altres nodes ni al datacenter.

---

### ğŸ§© **Cas 4: Hosting amb gestiÃ³ delegada per client**

#### Escenari:

Una empresa ofereix mÃ quines virtuals com a servei. Cada client gestiona la seua prÃ²pia mÃ quina.

#### ConfiguraciÃ³:

* **Client:** `client_a@pve`
* **Pool:** `client_a_pool`
* **VM assignada:** `vm201`
* **Rol:** `PVEVMUser`

#### Resultat:

Cada client pot administrar la seua prÃ²pia mÃ quina, sense cap visibilitat sobre altres clients o parts del sistema.

---

### âœ… Conclusions dels casos prÃ ctics

Aquests escenaris mostren com Proxmox permet adaptar-se fÃ cilment a entorns **multiusuari**, amb control granular de permisos i una gestiÃ³ segura i delegada, mantenint la **seguretat**, **eficiÃ¨ncia** i **flexibilitat** del sistema.

---