# 3. ğŸ–¥ï¸ ImplementaciÃ³ del ClÃºster Proxmox

A continuaciÃ³ et detallem pas a pas com crear un clÃºster en Proxmox i unir-hi altres nodes.

---

## ğŸ› ï¸ 1. Crear el ClÃºster

1. Accedeix a un dels nodes de Proxmox.
2. Ves a **Datacenter â†’ Cluster** des del menÃº lateral esquerre.
3. Fes clic a **Crear ClÃºster** (`Create Cluster`).

![Pantalla inicial del cluster](image.png)



4. Ompli les dades del clÃºster:

   * **Nom del ClÃºster**
   * **InterfÃ­cie de xarxa**
   * Altres parÃ metres segons la teua configuraciÃ³

<p align="center">
  <img src="image-1.png" alt="Crear cluster pas 1" />
</p>

<p align="center">
  <img src="image-2.png" alt="Crear cluster pas 2" />
</p>

5. Un cop creat, veurÃ s el node com a part del clÃºster.

![Cluster creat](image-3.png)

---

## ğŸ”— 2. Unir Nodes al ClÃºster

Per afegir un altre node al clÃºster:

1. Accedeix al segon node i ves a **Datacenter â†’ Cluster**.
2. Fes clic a **Unir-se al clÃºster** (`Join Cluster`).

![Unir-se al cluster](image-4.png)

3. A continuaciÃ³, haurÃ s dâ€™introduir la **informaciÃ³ del clÃºster**.

<p align="center">
  <img src="image-5.png" alt="Formulari unir-se" />
</p>

4. Per obtindre aquesta informaciÃ³, torna al node principal del clÃºster i fes clic a **Join Information**.

![InformaciÃ³ per unir-se](image-6.png)

5. Copia aquesta informaciÃ³ i torna al node secundari. Enganxa-la al formulari per unir-se.

<p align="center">
  <img src="image-7.png" alt="Enganxar informaciÃ³" />
</p>

6. Fes clic a **Unir-se**. Si tot Ã©s correcte, el node sâ€™afegirÃ  automÃ ticament al clÃºster.

![Node afegit](image-8.png)

---

## â• 3. Afegir mÃ©s nodes

Per afegir mÃ©s nodes, repeteix exactament el mateix procÃ©s:

* Accedeix al node
* Ves a **Datacenter â†’ Cluster**
* Fes clic a **Unir-se al clÃºster**
* Copia la informaciÃ³ del node principal
* Enganxa-la i uneix el node

---

ğŸ”š I amb aixÃ² ja tindrÃ s un clÃºster Proxmox funcional amb diversos nodes!

![ComprovaciÃ³](image-9.png)

Perfecte! Comencem pel punt **4.1 IntroducciÃ³ a Ceph i integraciÃ³ amb Proxmox**. Et deixe a continuaciÃ³ una proposta redactada en valenciÃ  formal, clara i adequada per al teu projecte:

---

### ğŸ§  4 IntroducciÃ³ a **Ceph** i IntegraciÃ³ amb **Proxmox VE**

**Ceph** Ã©s una plataforma dâ€™emmagatzematge distribuÃ¯t de codi obert dissenyada per oferir alta disponibilitat, escalabilitat i rendiment, sense punts Ãºnics de fallada. El seu funcionament es basa en tres components principals:

* **OSD (Object Storage Daemon):** Gestiona el disc dur on sâ€™emmagatzema la informaciÃ³.
* **MON (Monitor):** Controla lâ€™estat del clÃºster, mantÃ© el mapa del clÃºster i garanteix el consens entre nodes.
* **MGR (Manager):** Proporciona funcionalitats addicionals de monitoratge i interfÃ­cie web.

Ceph permet oferir emmagatzematge per a:

* MÃ quines virtuals (amb RBD â€“ Rados Block Device)
* Sistemes dâ€™arxius (CephFS)
* Objectes (compatible amb S3)

---

#### ğŸ”— IntegraciÃ³ amb Proxmox VE

**Proxmox VE** incorpora suport nadiu per a Ceph, cosa que facilita la seua instalÂ·laciÃ³, gestiÃ³ i integraciÃ³ des de la mateixa interfÃ­cie web o via lÃ­nia de comandes.

GrÃ cies a aquesta integraciÃ³:

* Es pot configurar Ceph directament des de la interfÃ­cie de **Datacenter â†’ Ceph**
* Els discos Ceph (RBD) poden ser utilitzats com a **emmagatzematge de mÃ quines virtuals** i **contenidors (LXC)**
* El sistema garanteix **alta disponibilitat**, ja que les dades estan replicades en diversos nodes
* Permet una **escala horitzontal** fÃ cil, afegint mÃ©s discos o nodes al clÃºster Ceph

---

ğŸ’¡ **Per quÃ¨ utilitzar Ceph en Proxmox?**

* Elimina la dependÃ¨ncia de sistemes dâ€™emmagatzematge extern (NFS, iSCSI, etc.)
* Millora la tolerÃ ncia a fallades i la continuÃ¯tat del servei
* Ofereix una gestiÃ³ centralitzada i unificada del clÃºster i lâ€™emmagatzematge

Perfecte! AcÃ­ tens el punt **4.2 InstalÂ·laciÃ³ i configuraciÃ³ de Ceph al clÃºster**, redactat en valenciÃ  formal i pensat per a un projecte tÃ¨cnic:

---

### 4.2 âš™ï¸ InstalÂ·laciÃ³ i ConfiguraciÃ³ de **Ceph** al ClÃºster

La instalÂ·laciÃ³ de Ceph en un entorn **Proxmox VE** es pot fer de manera centralitzada i senzilla grÃ cies a la seua integraciÃ³ nativa. A continuaciÃ³ es detallen els passos principals per a desplegar Ceph en un clÃºster de Proxmox:

---

#### ğŸ§© Requisits previs

Abans de comenÃ§ar amb la instalÂ·laciÃ³, cal assegurar:

* Tots els nodes tenen lâ€™hora sincronitzada via **NTP**
* Una xarxa especÃ­fica o VLAN dedicada per al trÃ nsit de Ceph (preferiblement amb baixa latÃ¨ncia i alta amplada de banda)
* Discos dedicats per a Ceph (no utilitzar el mateix disc que el sistema operatiu)
* Una configuraciÃ³ bÃ sica del clÃºster de Proxmox ja establida

---

#### ğŸ› ï¸ Passos dâ€™instalÂ·laciÃ³

1. **Accedir a la interfÃ­cie web de Proxmox**

   * Ves a `Datacenter â†’ Ceph`

2. **InstalÂ·lar Ceph als nodes**

   * Selecciona cada node on vols desplegar Ceph
   * A lâ€™apartat `Ceph`, fes clic a **Install Ceph**
   * El sistema instalÂ·larÃ  automÃ ticament els paquets necessaris (`ceph`, `ceph-common`, etc.)

![alt text](image-10.png)

![alt text](image-11.png)

3. **Crear els monitors (MON)**

   * Un mÃ­nim de **tres monitors** Ã©s recomanat per garantir el quorum
   * Des de lâ€™apartat `Monitor`, fes clic a **Create Monitor**

![alt text](image-12.png)

![alt text](image-13.png)

4. **Afegir el gestor (MGR)**

   * Necessari per a la interfÃ­cie grÃ fica i gestiÃ³ avanÃ§ada
   * Creaâ€™l des de la mateixa pestanya amb el botÃ³ **Create Manager**

![alt text](image-14.png)

5. **Afegir els OSDs (Object Storage Daemons)**

   * Els OSDs sÃ³n els processos que gestionen els discos durs del clÃºster
   * Ves a `OSD â†’ Create OSD`, selecciona el disc fÃ­sic i creaâ€™l
   * Repeteix el procÃ©s per a cada node i disc dedicat

![alt text](image-15.png)


<p align="center">
  <img src="image-16.png" alt="Enganxar informaciÃ³" />
</p>

* Com tenim 2 discos per cada node (menos en el node 3 que sols hi ha 1)de proxmox haurem de repetir el proccess dos voltes

**Node 1:**

![alt text](image-17.png)

**Node 2**

![alt text](image-18.png)

**Node 3**

![alt text](image-19.png)

6. **(Opcional) Crear un MDS (Metadata Server)**

   * NomÃ©s si vols utilitzar **CephFS** com a sistema de fitxers compartit

    ##### **ğŸ“‚ QuÃ¨ sÃ³n els metadades?**
    Els metadades sÃ³n informaciÃ³ sobre els fitxers, com ara:

    Noms de fitxers i directoris

    Jerarquia de carpetes

    Permisos dâ€™accÃ©s

    Propietaris

    Dates de creaciÃ³ o modificaciÃ³

    
    ##### ğŸ§  QuÃ¨ fa exactament el MDS?
    
    Quan utilitzes CephFS (el sistema de fitxers distribuÃ¯t de Ceph), el Metadata Server:

    Controla lâ€™estructura i organitzaciÃ³ del sistema de fitxers

    Processa operacions com ls, mkdir, rm, mv, etc.

    Fa que les consultes de fitxers siguen rÃ pides i escalables

    Allibera als OSDs dâ€™aquesta tasca perquÃ¨ es centren nomÃ©s en llegir i escriure dades

Perfecte! AcÃ­ tens el punt **4.3 CreaciÃ³ de pools dâ€™emmagatzematge** redactat de manera formal i clara en valenciÃ , seguint lâ€™estil dels punts anteriors:

---

### 4.3 ğŸ—ï¸ CreaciÃ³ de Pools dâ€™Emmagatzematge en Ceph

Els **pools dâ€™emmagatzematge** sÃ³n una part fonamental en lâ€™arquitectura de **Ceph**, ja que representen els espais lÃ²gics on es distribueixen les dades entre els diferents OSDs del clÃºster. A cada pool se li pot assignar una funciÃ³ especÃ­fica, com ara allotjar mÃ quines virtuals, contenidors o fitxers de CephFS.

---

### ğŸ” QuÃ¨ Ã©s un Pool?

Un **pool** Ã©s una agrupaciÃ³ lÃ²gica dâ€™objectes dins del clÃºster Ceph. Cada objecte dins dâ€™un pool es reparteix entre els OSDs segons una polÃ­tica de distribuciÃ³ definida, garantint aixÃ­ la replicaciÃ³ i la tolerÃ ncia a fallades.

---

### ğŸ› ï¸ CreaciÃ³ dâ€™un Pool pas a pas en Proxmox VE

1. Accedeix a la interfÃ­cie web de **Proxmox VE**
2. Ves a `Datacenter â†’ Ceph â†’ Pools`
3. Fes clic a **Create**

![alt text](image-20.png)

4. Emplena els camps segÃ¼ents:



   * **Nom del pool:** (ex. `vm_data`, `cephfs_data`, `backups`)
   * **Nombre de rÃ¨pliques (Size):** recomanat mÃ­nim **3** per a alta disponibilitat
   * **Min. rÃ¨pliques (Min. Size):** mÃ­nim **2** per a mantenir el servei actiu amb una fallada
   * **Crush Rule:** regla de distribuciÃ³ entre els dispositius de disc

<p align="center">
  <img src="image-21.png" alt="CreaciÃ³ de pool en Proxmox" />
</p>

1. Fes clic a **Create** i espera a que el pool aparega a la llista

![alt text](image-22.png)

Al pas d'un temps podem veure com en els nodes apareix l'almacenament del ceph.

![alt text](image-23.png)

---

### ğŸ§  Consideracions importants

* Els pools amb mÃ©s rÃ¨pliques consumeixen mÃ©s espai perÃ² ofereixen mÃ©s redundÃ ncia.
* Ã‰s possible crear **pools separats** per a diferents usos (ex: un per a VM i un altre per a CephFS).
* Es poden utilitzar **regles CRUSH** per controlar com es distribueixen les dades per racks, discos o ubicacions fÃ­siques.

---

### âœ… Resultat

Amb el pool creat, ja pots:

* Assignar-lo com a **emmagatzematge de mÃ quines virtuals (RBD)**
* Utilitzar-lo per a **CephFS**
* Monitorar el seu estat i Ãºs des de la pestanya de **Ceph â†’ Pools**

Perfecte! A continuaciÃ³ et presente el punt **4.4 Proves de rendiment i replicaciÃ³** redactat de manera formal i clara, mantenint lâ€™estil del teu projecte en valenciÃ :

---

### 4.4 ğŸš€ Proves de Rendiment i ReplicaciÃ³ en Ceph

Una vegada el clÃºster Ceph estÃ  desplegat i operatiu, Ã©s fonamental realitzar proves de **rendiment** i **replicaciÃ³** per a verificar el correcte funcionament de lâ€™emmagatzematge distribuÃ¯t, aixÃ­ com garantir la **fiabilitat** i **eficiÃ¨ncia** del sistema.

---

### ğŸ“Š Proves de rendiment

Les proves de rendiment ens permeten mesurar la **velocitat de lectura i escriptura** dels dispositius Ceph, aixÃ­ com la **latÃ¨ncia** i **capacitat de resposta** del clÃºster.

#### ğŸ§ª Eines recomanades:

* `rados bench` â†’ eina prÃ²pia de Ceph per mesurar el rendiment de lectura/escriptura
* `fio` â†’ eina externa per fer proves personalitzades dâ€™I/O

#### Exemple amb `rados bench`:

```bash
# Prova d'escriptura durant 60 segons
rados bench -p vm-data 60 write --no-cleanup

# Prova de lectura seqÃ¼encial
rados bench -p vm-data 60 seq

# Prova de lectura aleatÃ²ria
rados bench -p vm-data 60 rand
```

<p align="center">
  <img src="image-24.png" alt="CreaciÃ³ de pool en Proxmox" />
</p>

### âœ… Resultat esperat

* Les proves dâ€™escriptura i lectura han de mostrar valors de rendiment estables i adequats segons el teu hardware.
* La replicaciÃ³ ha de funcionar de manera automÃ tica, garantint la integritat i disponibilitat de les dades davant qualsevol fallada.

#### ğŸ“Œ Exemple de comandes via CLI

Alternativament, es pot fer la instalÂ·laciÃ³ via lÃ­nia de comandes:

```bash
# InstalÂ·lar Ceph
pveceph install

# Crear un monitor
pveceph mon create

# Crear un OSD
pveceph osd create /dev/sdX
```

---

### âœ… Resultat

Una vegada configurats els **MON**, **MGR** i **OSD**, el clÃºster Ceph estarÃ  operatiu. Ja pots procedir a:

* Crear **pools dâ€™emmagatzematge**
* Assignar-los com a backend per a mÃ quines virtuals
* Monitorar lâ€™estat del clÃºster des de la interfÃ­cie de Proxmox

---

### â™»ï¸ Proves de replicaciÃ³

Ceph replica les dades entre OSDs segons la configuraciÃ³ de rÃ¨pliques (per defecte 3). Ã‰s important verificar que:

1. **Les dades es repliquen correctament** a mÃºltiples discos.
2. **Quan cau un OSD o node**, Ceph automÃ ticament redistribueix les rÃ¨pliques.

#### ğŸ”„ Prova de fallada simulada:

1. Apaga un OSD manualment:

   ```bash
   systemctl stop ceph-osd@X
   ```

   (on `X` Ã©s el nÃºmero de lâ€™OSD)


2. Observa com Ceph reporta lâ€™estat *degraded* i com reubica les dades.

![Observar Ceph](image-25.png)

3. Torna a engegar lâ€™OSD i comprova la **reestructuraciÃ³ automÃ tica**:

   ```bash
   systemctl start ceph-osd@X
   ```

![RestauraciÃ³](image-26.png)

---
Perfecte! A continuaciÃ³ et redacte el punt **4.5 GestiÃ³ i monitoratge de Ceph**, amb to tÃ¨cnic, formal i en valenciÃ , adaptat per al teu projecte de forma clara i estructurada:

---

### ğŸ“ˆ 4.5 GestiÃ³ i Monitoratge de **Ceph**

Una vegada desplegat el clÃºster **Ceph**, Ã©s fonamental realitzar una gestiÃ³ i monitoratge continuat per garantir lâ€™estabilitat, el rendiment i la disponibilitat de les dades. Proxmox VE ofereix una **integraciÃ³ nativa amb Ceph**, que facilita tant el control operatiu com la detecciÃ³ anticipada de possibles incidÃ¨ncies.

---

### ğŸ› ï¸ Eines de gestiÃ³ disponibles

Proxmox proporciona diversos mÃ¨todes per gestionar Ceph:

#### ğŸ“Œ InterfÃ­cie web de Proxmox VE

Des de la GUI es pot accedir a:

* **Estat del clÃºster**: `Datacenter â†’ Ceph â†’ Status`
* **GestiÃ³ dâ€™OSDs**: afegir, eliminar o veure lâ€™estat dels Object Storage Daemons
* **Monitors (MONs)** i **Managers (MGRs)**: estat, creaciÃ³ i falles
* **CreaciÃ³ i eliminaciÃ³ de pools**
* **ConfiguraciÃ³ i gestiÃ³ de CephFS**

#### ğŸ”§ LÃ­nia de comandes (CLI)

Per a gestiÃ³ avanÃ§ada i automatitzacions:

```bash
ceph status             # Estat general del clÃºster
ceph osd tree           # VisualitzaciÃ³ jerÃ rquica dels OSDs
ceph df                 # Ãšs dâ€™espai en pools
ceph health detail      # InformaciÃ³ detallada de salut
ceph osd out/in <id>    # Marcar un OSD com a fora o dins del clÃºster
```

---

### ğŸ“Š Monitoratge actiu del clÃºster

Els principals parÃ metres a controlar de forma contÃ­nua sÃ³n:

* **Salut del clÃºster (HEALTH\_OK / WARN / ERR)**
* **Nombre dâ€™OSDs actius i en lÃ­nia**
* **Estat dels MON i MGR**
* **Ãšs de lâ€™espai per pool i per OSD**
* **LatÃ¨ncies de lectura i escriptura**
* **Rebalanceig de dades en cas de fallada o afegit de nous discos**

---

### ğŸ“¡ IntegraciÃ³ amb eines externes

Encara que Proxmox proporciona visualitzaciÃ³ bÃ sica, pots integrar Ceph amb eines de monitoratge mÃ©s potents com:

* **Prometheus + Grafana** (via Ceph MGR modules)
* **Zabbix** o **Nagios**, mitjanÃ§ant plugins
* Alertes per correu electrÃ²nic o sistemes de notificaciÃ³

---

### ğŸ’¡ Recomanacions de manteniment

* ğŸ” **Revisar lâ€™estat del clÃºster regularment**
* ğŸ§ª **Simular fallades controlades** per validar la replicaciÃ³ i recuperaciÃ³
* ğŸ“¦ **No sobrecarregar els OSDs**; mantindre un marge de capacitat lliure
* ğŸ›‘ **Evitar la pÃ¨rdua simultÃ nia de mÃºltiples discos** amb rÃ¨pliques mÃ­nimes

---

### âœ… Resultat

Amb una correcta gestiÃ³ i monitoratge, es garanteix que el clÃºster Ceph oferisca un rendiment estable, altament disponible i resistent a fallades, adaptat a les necessitats de lâ€™entorn virtualitzat en Proxmox VE.


Perfecte! AcÃ­ tens el punt **5. Alta Disponibilitat (HA)** completament redactat en valenciÃ  formal i tÃ¨cnic, amb icones i una estructura clara per a incloureâ€™l al teu projecte:

---

## ğŸ›¡ï¸ 5. Alta Disponibilitat (**HA**) en Proxmox VE

La **Alta Disponibilitat (High Availability)** Ã©s una funcionalitat clau en entorns crÃ­tics, ja que permet garantir la **continuÃ¯tat del servei** davant la caiguda dâ€™un node. Amb Proxmox VE, aquesta caracterÃ­stica sâ€™integra de manera nativa quan es treballa amb un **clÃºster**.

---

### âš™ï¸ 5.1 ActivaciÃ³ del Gestor HA en Proxmox

Per a fer Ãºs de la funcionalitat HA, cal que:

1. El clÃºster estiga format per almenys **3 nodes** (per mantenir el **quorum**)
2. Els nodes tinguen **Corosync** i **pve-ha-crm** actius
3. Els recursos (VM o CT) estiguen ubicats en **storage compartit** o Ceph

#### ğŸ”„ Procediment:

* Ves a `Datacenter â†’ HA`
* Asseguraâ€™t que el **HA Manager** estÃ  actiu
* Cada node mostrarÃ  el seu estat (online, standby, etc.)

<p align="center">
  <img src="image-27.png" alt="Gestor HA" />
</p>

---

### ğŸ§© 5.2 DefiniciÃ³ de Grups HA

Els **grups HA** permeten organitzar i assignar mÃ quines virtuals o contenidors per a gestionar millor les polÃ­tiques de tolerÃ ncia a fallades.

#### CreaciÃ³ dâ€™un grup HA:

1. Ves a `Datacenter â†’ HA â†’ Groups`

![alt text](image-28.png)

1. Fes clic a **Create**
2. Assigna:

<p align="center">
  <img src="image-29.png" alt="Gestor HA" />
</p>

   * **Nom del grup**
   * **Nodes preferits** per executar el servei
   * **Prioritats** (per decidir on sâ€™hauria de migrar en cas de fallada)

DesprÃ©s, quan crees o edites una VM/CT, pots assignar-la a un grup HA.

---

### ğŸ” 5.3 Proves de TolerÃ ncia a Fallades (Failover de MÃ quines Virtuals)

Per assegurar el correcte funcionament de la configuraciÃ³ HA, Ã©s recomanable fer proves de **failover controlades**:

#### Escenari de prova:

1. Assigna una VM a un grup HA

<p align="center">
  <img src="image-30.png" alt="Gestor HA" />
</p>

2. Para o apaga un node manualment

![alt text](image-31.png)

3. Observa com la VM Ã©s **migrada automÃ ticament** a un altre node disponible
4. Verifica que el servei continua operatiu sense intervenciÃ³ manual

ğŸ” Es pot monitorar aquest procÃ©s des de `Datacenter â†’ HA â†’ Status`.

---

### ğŸ’¡ 5.4 Casos dâ€™Ãšs i RecuperaciÃ³ davant Caigudes de Nodes

Els entorns amb HA actiu poden recuperar-se de forma automÃ tica en diferents situacions:

* ğŸ”Œ **Fallada de hardware o energia** en un node
* ğŸ§¯ **Actualitzacions crÃ­tiques** que requereixen reinici
* âš™ï¸ **Errors de sistema** o problemes de rendiment greu

En cada cas:

* El sistema HA detecta la fallada
* MigraciÃ³ automÃ tica a nodes disponibles
* RestauraciÃ³ del servei amb **temps mÃ­nim dâ€™interrupciÃ³**

#### Exemple prÃ ctic:

Una mÃ quina virtual crÃ­tica (servidor web, base de dades, etc.) estÃ  configurada amb HA. Si el node cau inesperadament, aquesta VM es reinicia en un altre node en qÃ¼estiÃ³ de segons, garantint la continuÃ¯tat del servei.

---

### âœ… Resultat

Amb la configuraciÃ³ HA en Proxmox VE, es millora significativament la **resiliÃ¨ncia de la infraestructura virtualitzada**, assegurant que els serveis essencials estiguen disponibles **24/7**, fins i tot davant fallades greus.

---

Vols que continue amb la secciÃ³ **6. Proxmox Backup Server (PBS)** o vols que tâ€™ajude a fer una portada o resum del projecte?
