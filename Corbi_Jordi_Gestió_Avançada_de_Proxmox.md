<a name="top"></a>
> Jordi CorbÃ­ MicÃ³
> IES JAUME II EL JUST (Tavernes de la Valldiga) - Curs 2023/2025  
> Cicle: AdministraciÃ³ de Sistemes Informatics en Xarxa

# Projecte Final de Cicle Superior d'ASIR: GestiÃ³ AvanÃ§ada de Proxmox

## ğŸ“Œ DescripciÃ³

Aquest projecte es basa en la **implementaciÃ³ dâ€™una infraestructura virtualitzada** mitjanÃ§ant la plataforma **Proxmox Virtual Environment (Proxmox VE)**. Lâ€™objectiu principal Ã©s **desplegar un clÃºster dâ€™alta disponibilitat** que permeta **centralitzar la gestiÃ³ de mÃ quines virtuals (VMs) i contenidors (LXC)**, garantint alhora escalabilitat, eficiÃ¨ncia de recursos i tolerÃ ncia a fallades.

Per aconseguir-ho, sâ€™ha configurat un entorn complet que inclou:

* Un **clÃºster de Proxmox VE** amb diversos nodes interconnectats.
* Un sistema dâ€™**emmagatzematge distribuÃ¯t** mitjanÃ§ant **Ceph**, per assegurar la replicaciÃ³ de dades i la disponibilitat contÃ­nua.
* La integraciÃ³ de **Proxmox Backup Server (PBS)** com a sistema de cÃ²pia de seguretat centralitzada i amb retenciÃ³ intelÂ·ligent.
* La configuraciÃ³ dâ€™**Alta Disponibilitat (HA)** per a la continuÃ¯tat del servei en cas de caiguda de nodes.
* La monitoritzaciÃ³ amb **Netdata Cloud**, per obtindre visibilitat en temps real del rendiment i estat del sistema.
* La implementaciÃ³ de **mecanismes de seguretat**, com tallafocs, actualitzacions automatitzades i control dâ€™accÃ©s delegat.

Aquest entorn permet simular escenaris reals dâ€™administraciÃ³ de sistemes, facilitant la gestiÃ³ dels recursos, la protecciÃ³ de dades i lâ€™automatitzaciÃ³ de tasques, tot dins dâ€™un marc tÃ¨cnic robust i preparat per a la producciÃ³ o entorns educatius avanÃ§ats.

## ğŸ§± Estructura del projecte

```
Projecte_Proxmox/
â””â”€â”€ README.md
â”‚   â””â”€â”€ documentaciÃ³/
â”‚       â”‚ 
â”‚       â”œâ”€â”€ instalaciÃ³/
â”‚       â”‚   â”œâ”€â”€ proxmox/
â”‚       â”‚   â”‚   â”œâ”€â”€  README.md
â”‚       â”‚   â”‚   â””â”€â”€  install.pdf
â”‚       â”‚   â”œâ”€â”€ proxmox_backup/
â”‚       â”‚   â”‚   â”œâ”€â”€  README.md
â”‚       â”‚   â”‚   â””â”€â”€  install.pdf
â”‚       â”‚   â””â”€â”€ zabbix/
â”‚       â”‚       â”œâ”€â”€  README.md
â”‚       â”‚       â””â”€â”€  install.pdf
â”‚       â”‚
â”‚       â”‚
â”‚       â””â”€â”€ configuraciÃ³/
â”‚           â”œâ”€â”€ proxmox/
â”‚           â”‚   â”œâ”€â”€  README.md
â”‚           â”‚   â””â”€â”€  conf.pdf
â”‚           â”œâ”€â”€ proxmox backup server/
â”‚           â”‚   â”œâ”€â”€  README.md
â”‚           â”‚   â””â”€â”€  conf.pdf
â”‚           â””â”€â”€ zabbix/
â”‚               â”œâ”€â”€  README.md
â”‚               â””â”€â”€  conf.pdf
â”‚
â”‚
â”œâ”€â”€ img/
â””â”€â”€  README.md
```

## ğŸ“„ Contingut

- **DocumentaciÃ³/**: ContÃ© la memÃ²ria del projecte i els annexos amb informaciÃ³ detallada sobre la implementaciÃ³ i configuraciÃ³.
- **README.md**: Aquest fitxer, que proporciona una visiÃ³ general del projecte.

## âš™ï¸ Requisits

- Proxmox VE 8.x
- Proxmox Backup Server 
- Maquinari compatible amb virtualitzaciÃ³ (Intel VT-x o AMD-V)
- ConnexiÃ³ a Internet per a la descÃ rrega de paquets i actualitzacions

---

## ğŸ“˜ 1. IntroducciÃ³

### ğŸ”§ **QuÃ¨ Ã©s Proxmox VE?**

**Proxmox VE (Virtual Environment)** Ã©s una **plataforma de virtualitzaciÃ³ d'entorns oberts** basada en Debian GNU/Linux, orientada a la creaciÃ³ i gestiÃ³ de **mÃ quines virtuals (VMs)** i **contenidors (LXCs)** en entorns de producciÃ³.

Proxmox VE integra dues tecnologies principals de virtualitzaciÃ³:

* **KVM (Kernel-based Virtual Machine):** per a la virtualitzaciÃ³ completa de sistemes operatius.
* **LXC (Linux Containers):** per a la virtualitzaciÃ³ lleugera a nivell de sistema operatiu.

A mÃ©s, incorpora eines avanÃ§ades com:

* **GestiÃ³ de clÃºsters:** permet agrupar mÃºltiples nodes en una sola interfÃ­cie centralitzada.
* **Alta Disponibilitat (HA):** per a la migraciÃ³ automÃ tica de VMs/LXCs entre nodes en cas de fallada.
* **Ceph Storage:** sistema dâ€™emmagatzematge distribuÃ¯t integrat, tolerant a fallades i altament escalable.
* **Proxmox Backup Server (PBS):** per a cÃ²pies de seguretat eficients i amb deduplicaciÃ³.
* **GestiÃ³ de xarxes virtuals:** amb VLANs, ponts i interfÃ­cies virtuals.
* **InterfÃ­cie web intuÃ¯tiva i potent:** per gestionar tot el sistema des del navegador.

Proxmox VE Ã©s una soluciÃ³ de virtualitzaciÃ³ completa pensada tant per a entorns empresarials com acadÃ¨mics, oferint una alternativa robusta, gratuÃ¯ta i de codi obert a altres plataformes com VMware vSphere o Microsoft Hyper-V.

---

### ğŸ¯ 1.1 Objectius del projecte

Lâ€™objectiu principal dâ€™aquest projecte Ã©s dissenyar, desplegar i documentar una infraestructura virtualitzada dâ€™alta disponibilitat basada en **Proxmox VE**, enfocada tant a la resiliÃ¨ncia com a la gestiÃ³ eficient de recursos. El sistema es construeix sobre un clÃºster format per **tres nodes fÃ­sics** que ofereixen serveis de virtualitzaciÃ³ mitjanÃ§ant **KVM/QEMU**, amb funcionalitats avanÃ§ades de gestiÃ³ centralitzada.

Per garantir la disponibilitat i continuÃ¯tat del servei davant possibles fallades, sâ€™integra un sistema dâ€™**emmagatzematge distribuÃ¯t amb Ceph**, proporcionant replicaciÃ³ automÃ tica, escalabilitat i tolerÃ ncia a falles. Aquesta arquitectura permet que les mÃ quines virtuals es puguin migrar dinÃ micament entre nodes i continuÃ¯n funcionant fins i tot en cas de caiguda parcial de la infraestructura.

Com a part essencial del projecte, es desplega tambÃ© un **Proxmox Backup Server (PBS)** per gestionar de manera centralitzada les cÃ²pies de seguretat, amb una estratÃ¨gia definida de programaciÃ³, retenciÃ³ i restauraciÃ³ eficient de mÃ quines virtuals. AixÃ² assegura la recuperaciÃ³ rÃ pida davant d'incidÃ¨ncies, i millora la integritat i seguretat de les dades.

Lâ€™objectiu final Ã©s demostrar la viabilitat i robustesa dâ€™una soluciÃ³ de virtualitzaciÃ³ empresarial utilitzant tecnologies de codi obert, tot documentant-ne la planificaciÃ³, implementaciÃ³, proves de rendiment i mesures de seguretat, amb una orientaciÃ³ clara a lâ€™escalabilitat, la facilitat de manteniment i lâ€™alt rendiment operatiu.

---

### ğŸ§© 1.2 JustificaciÃ³ de lâ€™elecciÃ³ de Proxmox VE

Sâ€™ha triat **Proxmox VE (Virtual Environment)** com a plataforma base del projecte per la seua naturalesa de codi obert, la seua gran comunitat, i la capacitat dâ€™oferir una **soluciÃ³ integral de virtualitzaciÃ³** sense requerir llicÃ¨ncies comercials costoses. Proxmox combina potents tecnologies com **KVM (Kernel-based Virtual Machine)** per a la virtualitzaciÃ³ completa i **LXC (Linux Containers)** per a la virtualitzaciÃ³ lleugera, permetent adaptar-se a diversos escenaris dâ€™Ãºs amb eficiÃ¨ncia de recursos.

Una de les caracterÃ­stiques clau que ha motivat la seua elecciÃ³ Ã©s la **integraciÃ³ nativa amb Ceph**, un sistema dâ€™emmagatzematge distribuÃ¯t i tolerant a fallades, aixÃ­ com amb **Proxmox Backup Server (PBS)** per gestionar cÃ²pies de seguretat de manera centralitzada, incremental i deduplicada. A mÃ©s, Proxmox ofereix funcionalitats dâ€™**alta disponibilitat (HA)** amb gestiÃ³ automÃ tica del reinici de mÃ quines virtuals en cas de caiguda de nodes, aixÃ­ com **clustering** completament integrat mitjanÃ§ant `pvecm`.

Comparat amb altres plataformes de virtualitzaciÃ³, Proxmox destaca per:

* **VMware vSphere**: Tot i ser una de les solucions mÃ©s madures i utilitzades en entorns corporatius, implica **alts costos de llicenciament**, especialment si es desitja alta disponibilitat, emmagatzematge compartit o automatitzaciÃ³ amb vCenter. Proxmox, en canvi, ofereix funcionalitats similars sense cost de llicÃ¨ncia i amb un model dâ€™assistÃ¨ncia opcional.

* **Microsoft Hyper-V**: Encara que estÃ  integrat en sistemes Windows Server, la seua gestiÃ³ resulta **menys flexible en entorns mixtos Linux/Windows** i sovint requereix eines addicionals com System Center per oferir funcionalitats comparables a les de Proxmox. La integraciÃ³ amb Ceph o tecnologies de codi obert Ã©s limitada.

* **Red Hat Virtualization (RHV)**: Basat tambÃ© en KVM, RHV Ã©s una plataforma potent, perÃ² **requereix subscripcions comercials** i presenta una corba dâ€™aprenentatge mÃ©s elevada. Proxmox redueix la complexitat i facilita la posada en marxa mitjanÃ§ant una **interfÃ­cie web intuÃ¯tiva i unificada**, apta fins i tot per a perfils tÃ¨cnics intermedis.

Finalment, la disponibilitat de **documentaciÃ³ extensa**, suport de la comunitat i la **rapidesa en desplegament** fan de Proxmox VE una opciÃ³ ideal per a entorns educatius, laboratoris i pimes, sense renunciar a prestacions prÃ²pies dâ€™entorns empresarials. Aquesta versatilitat i autonomia en la gestiÃ³ de la infraestructura virtual han estat factors decisius per escollir-lo com a tecnologia base del projecte.

---

### ğŸ§­ 1.3 Abast del Projecte

Aquest projecte abasta de manera integral totes les fases necessÃ ries per al desplegament dâ€™una **infraestructura virtualitzada dâ€™alta disponibilitat**, utilitzant tecnologies de codi obert amb un enfocament prÃ ctic i escalable. La planificaciÃ³, implementaciÃ³ i documentaciÃ³ cobreixen tant la part fÃ­sica com la lÃ²gica del sistema, assegurant un entorn robust, segur i fÃ cilment administrable.

Les accions principals que formen part de lâ€™abast del projecte sÃ³n:

* **Disseny i desplegament de tres nodes fÃ­sics** amb **Proxmox VE**, configurats en mode **clÃºster** per oferir gestiÃ³ centralitzada, suport a lâ€™alta disponibilitat i funcionalitats avanÃ§ades com la migraciÃ³ en viu de mÃ quines virtuals.

* **IntegraciÃ³ completa amb Ceph** com a sistema dâ€™**emmagatzematge distribuÃ¯t**, aprofitant la seua capacitat de replicaciÃ³, resiliÃ¨ncia i escalabilitat per garantir la persistÃ¨ncia de les dades i la disponibilitat del servei davant falles de maquinari.

* **InstalÂ·laciÃ³ i configuraciÃ³ de Proxmox Backup Server (PBS)** per implementar una **estratÃ¨gia automatitzada de cÃ²pies de seguretat**, amb suport per backups incrementals, deduplicaciÃ³ i restauraciÃ³ eficient de mÃ quines virtuals.

* **DefiniciÃ³ i aplicaciÃ³ dâ€™una arquitectura dâ€™alta disponibilitat (HA)**, incloent mecanismes de failover automatitzat, agrupaciÃ³ de recursos i comprovacions de tolerÃ ncia a fallades a nivell de node i emmagatzematge.

* **ConfiguraciÃ³ de rols, usuaris i polÃ­tiques de seguretat**, incloent la gestiÃ³ dâ€™accÃ©s, permisos granulars i monitoritzaciÃ³ dâ€™activitats, tot garantint la seguretat i el control de lâ€™entorn virtualitzat.

* **ElaboraciÃ³ de documentaciÃ³ tÃ¨cnica detallada**, incloent manuals dâ€™instalÂ·laciÃ³ pas a pas, guies dâ€™administraciÃ³ del clÃºster, procediments de recuperaciÃ³ davant incidÃ¨ncies i instruccions dâ€™Ãºs per a usuaris delegats.

Aquest abast garanteix no nomÃ©s la posada en marxa del sistema, sinÃ³ tambÃ© la seua operativitat i manteniment a llarg termini, assegurant la continuÃ¯tat del servei i la capacitat de resposta davant imprevistos. A mÃ©s, sâ€™ha tingut en compte la possibilitat dâ€™escalabilitat futura per afegir nous nodes o serveis al clÃºster.

---

### ğŸ§­ 1.4 Requisits Previs i Coneixements Necessaris

Per tal de dur a terme amb Ã¨xit aquest projecte dâ€™infraestructura virtualitzada amb alta disponibilitat, Ã©s imprescindible disposar dâ€™uns **coneixements previs sÃ²lids** en diverses Ã rees tÃ¨cniques relacionades amb sistemes, virtualitzaciÃ³ i administraciÃ³ de xarxes. Aquests coneixements permeten no nomÃ©s la correcta implementaciÃ³ de les tecnologies involucrades, sinÃ³ tambÃ© la resoluciÃ³ eficient de problemes i lâ€™optimitzaciÃ³ de lâ€™entorn.

Els requisits tÃ¨cnics principals inclouen:

* **AdministraciÃ³ de sistemes Linux**, especialment en entorns basats en **Debian** (sistema base de Proxmox VE). Es requereix fluÃ¯desa en tasques com gestiÃ³ de serveis, permisos, xarxes, i lâ€™Ãºs dâ€™eines habituals dâ€™administraciÃ³.

* **Coneixements de virtualitzaciÃ³**, tant en entorns de mÃ quines virtuals amb **KVM/QEMU**, com en **contenidors LXC**, ja que Proxmox VE permet desplegar i gestionar ambdÃ³s tipus dâ€™instÃ ncies.

* **Conceptes fonamentals dâ€™emmagatzematge distribuÃ¯t**, amb especial Ã¨mfasi en **Ceph**: arquitectura, tipus de nodes (monitor, OSD, MDS), principis de replicaciÃ³, pools i gestiÃ³ de recursos. Tot i que el projecte no requereix una profunditat mÃ xima, sÃ­ que Ã©s necessari entendre el seu funcionament bÃ sic per implementar-lo correctament dins dâ€™un clÃºster.

* **GestiÃ³ dâ€™usuaris, rols i permisos**, aixÃ­ com lâ€™aplicaciÃ³ de **polÃ­tiques de seguretat**, incloent lâ€™Ãºs de tallafocs (com el propi sistema de Proxmox), accÃ©s SSH, autenticaciÃ³ i segregaciÃ³ dâ€™usuaris amb permisos diferenciats.

* **Habilitat amb la lÃ­nia dâ€™ordres (CLI) en Linux**, especialment per a la configuraciÃ³ directa dâ€™elements com `pvecm`, `ceph`, configuraciÃ³ de xarxes i ediciÃ³ dâ€™arxius com `/etc/network/interfaces`, `/etc/pve/` o fitxers de servei. El projecte combina tant interfÃ­cie grÃ fica com administraciÃ³ per consola.

A mÃ©s, es valora tenir coneixements generals en:

* Xarxes IP (subxarxes, VLANs, ponts de xarxa)
* Monitoratge de sistemes
* CÃ²pies de seguretat i estratÃ¨gies de retenciÃ³

Aquest conjunt de coneixements assegura que lâ€™usuari o equip executor puga afrontar amb autonomia la planificaciÃ³, el desplegament i la gestiÃ³ operativa dâ€™una infraestructura virtualitzada basada en Proxmox VE.

---

## ğŸ§± 2. AnÃ lisi i Disseny de la Infraestructura

Lâ€™objectiu dâ€™aquesta secciÃ³ Ã©s definir amb detall els **requisits funcionals i tÃ¨cnics**, la **topologia de xarxa** i el **disseny lÃ²gic** de la infraestructura necessÃ ria per desplegar un **clÃºster Proxmox VE amb alta disponibilitat**, integrant tant un **sistema dâ€™emmagatzematge distribuÃ¯t Ceph** com una **soluciÃ³ de cÃ²pia de seguretat centralitzada amb Proxmox Backup Server (PBS)**.

Aquest apartat proporciona una visiÃ³ global dels components essencials, establint les bases per a una implementaciÃ³ robusta, escalable i tolerable a fallades. Lâ€™arquitectura proposada respon a criteris dâ€™eficiÃ¨ncia, resiliÃ¨ncia i seguretat, assegurant la continuÃ¯tat del servei fins i tot davant de falles parcials del sistema.

Els punts que es desenvolupen en aquesta secciÃ³ sÃ³n:

* **2.1 Requisits funcionals i no funcionals**
  IdentificaciÃ³ dels objectius tÃ¨cnics (funcionals) com la creaciÃ³ del clÃºster, lâ€™alta disponibilitat i la integraciÃ³ de Ceph, aixÃ­ com requisits no funcionals com el rendiment, la seguretat i la facilitat de manteniment del sistema.

* **2.2 Topologia de xarxa proposada**
  Disseny detallat de la infraestructura de xarxa, incloent VLANs si escau, interfÃ­cies dedicades per a gestiÃ³, sincronitzaciÃ³ del clÃºster i trÃ fic Ceph, garantint una segmentaciÃ³ Ã²ptima del trÃ fic i minimitzant colls dâ€™ampolla.

* **2.3 Maquinari utilitzat**
  DescripciÃ³ dels nodes fÃ­sics (CPU, RAM, discs, interfÃ­cies de xarxa), aixÃ­ com del maquinari emprat per a la infraestructura de suport (switches, servidor de backups, etc.).

* **2.4 Disseny lÃ²gic del clÃºster Proxmox**
  ExplicaciÃ³ de com es configura el clÃºster a nivell lÃ²gic: estructura del *cluster quorum*, configuraciÃ³ dels nodes, agrupaciÃ³ de recursos i distribuciÃ³ de cÃ rregues.

* **2.5 Consideracions dâ€™alta disponibilitat i tolerÃ ncia a fallades**
  AnÃ lisi de lâ€™estratÃ¨gia HA implementada, com la definiciÃ³ de grups HA, la gestiÃ³ automÃ tica de reinicis de mÃ quines virtuals i la tolerÃ ncia davant la pÃ¨rdua dâ€™un node o del sistema dâ€™emmagatzematge.

Aquest capÃ­tol Ã©s fonamental per garantir que el desplegament posterior es realitze sobre una base ben definida, coherent i alineada amb les necessitats del projecte. Un disseny acurat minimitza riscos, facilita la gestiÃ³ a llarg termini i assegura una millor resposta davant incidÃ¨ncies.

---

### âœ… 2.1 Requisits Funcionals i No Funcionals

#### **Requisits funcionals**

* El sistema ha de permetre la creaciÃ³ i gestiÃ³ de mÃ quines virtuals i contenidors (VM i LXC).
* Ha dâ€™incloure un sistema dâ€™emmagatzematge distribuÃ¯t (Ceph).
* Sâ€™ha de poder realitzar cÃ²pies de seguretat automÃ tiques mitjanÃ§ant Proxmox Backup Server.
* El clÃºster ha de suportar alta disponibilitat per a serveis crÃ­tics.
* Sâ€™ha de poder gestionar usuaris amb permisos delegats.

#### **Requisits no funcionals**

* La infraestructura ha de ser escalable per a afegir nous nodes o recursos.
* Ha de tindre tolerÃ ncia a fallades sense pÃ¨rdua de dades.
* Ha dâ€™oferir un rendiment acceptable amb maquinari limitat.
* El sistema ha de ser administrable mitjanÃ§ant una interfÃ­cie grÃ fica web intuÃ¯tiva.

---

### ğŸŒ 2.2 Topologia de Xarxa Proposada

> En aquest entorn de prÃ ctiques sâ€™ha desplegat un Ãºnic servidor fÃ­sic amb **Proxmox VE** com a hipervisor principal. Dins dâ€™aquest servidor, sâ€™han creat diverses mÃ quines virtuals que simulen els diferents **nodes dâ€™un clÃºster**, aixÃ­ com un servidor addicional amb **Proxmox Backup Server (PBS)**.
>
> Aquesta arquitectura permet **reproduir un escenari realista** amb alta disponibilitat, emmagatzematge distribuÃ¯t (Ceph) i cÃ²pies de seguretat centralitzades, perÃ² en un entorn virtualitzat controlat i sense necessitat de diversos equips fÃ­sics.

---

### ğŸ–¥ï¸ Diagrama:

```plaintext
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚         Servidor fÃ­sic amb Proxmox VE                                 â”‚
             â”‚                                                                       â”‚
             â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
             â”‚   â”‚  Node 1 VM â”‚ â†â†’â”‚  Node 2 VM â”‚ â†â†’â”‚  Node 3 VM â”‚   â”‚   PBS VM   â”‚   â”‚
             â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
             â”‚                                                                       â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ”§ *Tots els nodes i el PBS sÃ³n mÃ quines virtuals creades dins del mateix host Proxmox VE.*

---

### ğŸ–¥ï¸ 2.3 Maquinari Utilitzat

#### **Nodes del clÃºster (x2):**

* **CPU:** 16 x Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz img
* **RAM:** 32 GB DDR4
* **Disc SSD:** 1x 150 GB per a sistema
* **Discos HDD:** 2x 100 GB per a Ceph (OSD)

#### **Nodes del clÃºster 3:**

* **CPU:** 12 x Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz 
* **RAM:** 24 GB DDR4
* **Disc SSD:** 1 x 150 GB per a sistema
* **Discos HDD:** 1 x 100 GB per a Ceph (OSD)

#### **Proxmox Backup Server:**

* **CPU:** 16 x Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz 
* **RAM:** 32 GB
* **Disc SSD:** 1 x 150 GB per al sistema
* **HDD:** 3 x 100 GB RAID1 (datastore de cÃ²pies)

---

### ğŸ’° Pressupost Estimat dâ€™Infraestructura per a ClÃºster Proxmox amb HA, Ceph i PBS

#### ğŸ–¥ï¸ **Nodes del ClÃºster (x3)**

*Servidors fÃ­sics amb suport per a virtualitzaciÃ³, alta disponibilitat i Ceph*

| Component                                                           | Quantitat | Preu unitari aprox. | Subtotal    |
| ------------------------------------------------------------------- | --------- | ------------------- | ----------- |
| Servidor Proxmox VE (CPU 16 cores, 32-64 GB RAM, SSD + 2x HDD 4 TB) | 3         | 1.200â€¯â‚¬             | 3.600â€¯â‚¬     |
| Targetes de xarxa addicionals (1/10 Gb) + cablejat                  | 3         | 100â€¯â‚¬               | 300â€¯â‚¬       |
| **Subtotal nodes del clÃºster**                                      |           |                     | **3.900â€¯â‚¬** |

---

#### ğŸ’¾ **Servidor de Proxmox Backup Server (PBS)**

*Servidor dedicat per a cÃ²pies de seguretat amb alta capacitat i fiabilitat*

| Component                                                       | Quantitat | Preu unitari aprox. | Subtotal    |
| --------------------------------------------------------------- | --------- | ------------------- | ----------- |
| Servidor PBS (CPU 16 cores, 32 GB RAM, SSD + 3x HDD 4 TB RAID)  | 1         | 1.100â€¯â‚¬             | 1.100â€¯â‚¬     |
| Unitat externa d'emmagatzematge (opcional per backups off-site) | 1         | 300â€¯â‚¬               | 300â€¯â‚¬       |
| **Subtotal PBS**                                                |           |                     | **1.400â€¯â‚¬** |

---

#### ğŸŒ **Infraestructura de Xarxa i Accessoris**

| Component                                  | Quantitat | Preu unitari aprox. | Subtotal  |
| ------------------------------------------ | --------- | ------------------- | --------- |
| Switch gestionable gigabit/10Gb            | 1         | 400â€¯â‚¬               | 400â€¯â‚¬     |
| SAI (Sistema dâ€™alimentaciÃ³ ininterrompuda) | 1         | 300â€¯â‚¬               | 300â€¯â‚¬     |
| Bastidor (rack) i accessoris               | 1         | 250â€¯â‚¬               | 250â€¯â‚¬     |
| **Subtotal xarxa/accessoris**              |           |                     | **950â€¯â‚¬** |

---

### ğŸ“„ **Total Pressupost Estimat**

| Part                            | Cost aproximat |
| ------------------------------- | -------------- |
| Nodes del clÃºster (x3)          | 3.900â€¯â‚¬        |
| Proxmox Backup Server (PBS)     | 1.400â€¯â‚¬        |
| Infraestructura de xarxa i rack | 950â€¯â‚¬          |
| **TOTAL GENERAL**               | **\~6.250â€¯â‚¬**  |

---

### ğŸ§¾ Notes finals:

* Els preus inclouen maquinari amb capacitat real per executar entorns Ceph i HA amb garantia de rendiment.
* Es poden reduir costos amb equips refurbished o dâ€™ocasiÃ³, perÃ² aquest pressupost reflecteix una configuraciÃ³ professional i realista.
* No sâ€™han inclÃ²s llicÃ¨ncies comercials opcionals de Proxmox (el programari Ã©s lliure, perÃ² el suport Ã©s de pagament si es desitja).

---

### ğŸ§© 2.4 Disseny LÃ²gic del ClÃºster Proxmox

El disseny lÃ²gic del clÃºster estÃ  orientat a garantir **alta disponibilitat, rendiment i escalabilitat**, aprofitant les funcionalitats natives de **Proxmox VE** i la seua integraciÃ³ directa amb **Ceph** com a plataforma dâ€™emmagatzematge distribuÃ¯t.

El clÃºster estarÃ  compost per **tres nodes fÃ­sics** amb Proxmox VE, cadascun dels quals assumeix rols crÃ­tics dins de la infraestructura:

* **Rols assignats per node:**

  * Cada node actuarÃ  simultÃ niament com a **MON (Monitor)** per al manteniment del consens i del mapa del clÃºster Ceph.
  * Tindran tambÃ© el rol de **MGR (Manager)** per facilitar el monitoratge i la interfÃ­cie dâ€™administraciÃ³ del Ceph.
  * A mÃ©s, exerciran com a **OSD (Object Storage Daemon)**, aprofitant discos dedicats per oferir emmagatzematge distribuÃ¯t i replicat entre nodes.

Aquest model distribuÃ¯t assegura que el clÃºster siga funcional i operatiu fins i tot si un dels nodes queda fora de servei, mantenint el **quÃ²rum** necessari tant per a Ceph com per al propi clÃºster de Proxmox.

* **Pool dâ€™emmagatzematge:**

  * Es crearÃ  un **pool de tipus Ceph RBD (RADOS Block Device)**, destinat especÃ­ficament a allotjar mÃ quines virtuals i contenidors. Aquest pool garanteix redundÃ ncia mitjanÃ§ant rÃ¨pliques i ofereix accÃ©s rÃ pid i distribuÃ¯t a les dades.

* **Alta disponibilitat (HA):**

  * Les mÃ quines virtuals considerades crÃ­tiques es configuraran amb **HA**, de manera que, en cas de fallada dâ€™un node, aquestes es reinicien automÃ ticament en un altre node disponible sense intervenciÃ³ manual.
  * Es definiran regles de preferÃ¨ncia i prioritat per optimitzar la distribuciÃ³ de cÃ rrega i garantir la resposta immediata davant dâ€™incidÃ¨ncies.

* **GestiÃ³ centralitzada:**

  * Lâ€™administraciÃ³ del clÃºster es durÃ  a terme mitjanÃ§ant la **interfÃ­cie web integrada de Proxmox VE**, que permet la gestiÃ³ completa de nodes, mÃ quines virtuals, recursos, backups i emmagatzematge, tot des dâ€™un Ãºnic punt dâ€™accÃ©s.
  * A mÃ©s, es mantindrÃ  accÃ©s per lÃ­nia dâ€™ordres (`pvecm`, `ceph`, `qm`, etc.) per a tasques avanÃ§ades o scripts dâ€™automatitzaciÃ³.

Aquest disseny garanteix un **entorn equilibrat, resilient i fÃ cil de gestionar**, optimitzat per a oferir serveis ininterromputs, adaptable a lâ€™escalat futur i alineat amb les millors prÃ ctiques de virtualitzaciÃ³ en entorns empresarials.

---

### ğŸ›¡ï¸ 2.5 Consideracions dâ€™Alta Disponibilitat i TolerÃ ncia a Fallades

Lâ€™arquitectura proposada ha estat dissenyada per oferir **alta disponibilitat (HA)** i **tolerÃ ncia a fallades**, garantint aixÃ­ la continuÃ¯tat dels serveis virtualitzats davant de caigudes parcials del sistema. La combinaciÃ³ de tecnologies com **Proxmox VE, Ceph i Proxmox Backup Server (PBS)** permet una resposta automÃ tica, eficient i segura davant incidÃ¨ncies crÃ­tiques.

Els principals mecanismes de disponibilitat sÃ³n:

* **Corosync per a comunicaciÃ³ i manteniment del quÃ²rum:**
  El sistema utilitza **Corosync** per sincronitzar lâ€™estat dels nodes i mantenir el **quÃ²rum** del clÃºster. Aquest protocol de missatgeria distribuÃ¯da detecta falles de nodes i pren decisions de gestiÃ³ segons la disponibilitat dels membres del clÃºster.

* **Ceph com a emmagatzematge replicat i tolerant a fallades:**
  Ceph replica automÃ ticament les dades entre mÃºltiples **OSDs** distribuÃ¯ts als tres nodes. Aquesta replicaciÃ³ (amb un mÃ­nim de 3 cÃ²pies per objecte) assegura que, en cas de fallada dâ€™un disc o node, no es perd informaciÃ³ i es mantÃ© la integritat del sistema.

* **MÃ²dul HA integrat en Proxmox VE:**
  El subsistema HA permet definir **grups dâ€™alta disponibilitat** per a mÃ quines virtuals i contenidors crÃ­tics. Si un node falla, el mÃ²dul HA **migra o reinicia automÃ ticament** les mÃ quines afectades en un altre node actiu del clÃºster, minimitzant el temps dâ€™inactivitat.

* **Estructura amb 3 nodes per mantenir el clÃºster operatiu amb fallada dâ€™un node:**
  El disseny amb **tres nodes fÃ­sics** assegura que es mantinga el **quÃ²rum majoritari (2 de 3)** fins i tot si un node deixa de funcionar. AixÃ² permet continuar operant amb normalitat i evitar situacions de *split-brain* o inconsistÃ¨ncies.

* **Servidor de cÃ²pies de seguretat fora del clÃºster:**
  El **Proxmox Backup Server** sâ€™instalÂ·la en un host separat del clÃºster per garantir una **cÃ²pia externa segura** de totes les mÃ quines virtuals i configuracions. En cas de fallada catastrÃ²fica del clÃºster, aquest servidor permet recuperar rÃ pidament lâ€™estat del sistema sense dependre de la infraestructura fallida.

Aquesta estratÃ¨gia global dâ€™alta disponibilitat i resiliÃ¨ncia proporciona un entorn fiable i apte per a entorns de producciÃ³, minimitzant tant els riscos de pÃ¨rdua de dades com els temps dâ€™interrupciÃ³ dels serveis.

---

# 4. ğŸ§© ConfiguraciÃ³ de Ceph com a Emmagatzematge DistribuÃ¯t

### ğŸ§  4.1 IntroducciÃ³ a **Ceph** i IntegraciÃ³ amb **Proxmox VE**

**Ceph** Ã©s una plataforma dâ€™emmagatzematge distribuÃ¯t de codi obert dissenyada per oferir alta disponibilitat, escalabilitat i rendiment, sense punts Ãºnics de fallada. El seu funcionament es basa en tres components principals:

* **OSD (Object Storage Daemon):** Gestiona el disc dur on sâ€™emmagatzema la informaciÃ³.
* **MON (Monitor):** Controla lâ€™estat del clÃºster, mantÃ© el mapa del clÃºster i garanteix el consens entre nodes.
* **MGR (Manager):** Proporciona funcionalitats addicionals de monitoratge i interfÃ­cie web.

Ceph permet oferir emmagatzematge per a:

* MÃ quines virtuals (amb RBD â€“ Rados Block Device)
* Sistemes dâ€™arxius (CephFS)
* Objectes (compatible amb S3)

---

#### ğŸ”— IntegraciÃ³ amb Proxmox VE

**Proxmox VE** incorpora suport nadiu per a Ceph, cosa que facilita la seua instalÂ·laciÃ³, gestiÃ³ i integraciÃ³ des de la mateixa interfÃ­cie web o via lÃ­nia de comandes.

GrÃ cies a aquesta integraciÃ³:

* Es pot configurar Ceph directament des de la interfÃ­cie de **Datacenter â†’ Ceph**
* Els discos Ceph (RBD) poden ser utilitzats com a **emmagatzematge de mÃ quines virtuals** i **contenidors (LXC)**
* El sistema garanteix **alta disponibilitat**, ja que les dades estan replicades en diversos nodes
* Permet una **escala horitzontal** fÃ cil, afegint mÃ©s discos o nodes al clÃºster Ceph

---

ğŸ’¡ **Per quÃ¨ utilitzar Ceph en Proxmox?**

* Elimina la dependÃ¨ncia de sistemes dâ€™emmagatzematge extern (NFS, iSCSI, etc.)
* Millora la tolerÃ ncia a fallades i la continuÃ¯tat del servei
* Ofereix una gestiÃ³ centralitzada i unificada del clÃºster i lâ€™emmagatzematge

# 5. ğŸ›¡ï¸ Alta Disponibilitat (HA)

Lâ€™**Alta Disponibilitat (HA)** Ã©s un conjunt de tecnologies i configuracions dissenyades per garantir que els serveis crÃ­tics dâ€™un sistema **romanguen operatius de manera contÃ­nua**, fins i tot davant fallades de maquinari, programari o xarxa. En entorns virtualitzats com **Proxmox VE**, la funcionalitat HA Ã©s essencial per assegurar la **mÃ­nima interrupciÃ³ dels serveis** que allotgen mÃ quines virtuals i contenidors.

#### ğŸ¯ Finalitat de la HA:

L'objectiu principal de la HA Ã©s **reduir al mÃ xim el temps dâ€™inactivitat (downtime)**. Quan un servidor fÃ­sic (node) del clÃºster deixa de funcionar â€”ja siga per avaria, reinici o manteniment imprevistâ€”, el sistema HA detecta automÃ ticament la fallada i **reinicia les mÃ quines virtuals afectades en un altre node actiu** del clÃºster, sense intervenciÃ³ manual.

#### âš™ï¸ Funcionament dins de Proxmox VE:

Proxmox VE incorpora un subsistema HA que treballa estretament amb **Corosync**, el qual sâ€™encarrega de supervisar la salut dels nodes i mantenir el quÃ²rum del clÃºster. Les mÃ quines virtuals que es volen protegir es configuren dins de **grups HA**, i el gestor HA pren decisions automÃ tiques segons lâ€™estat dels nodes.

El sistema HA inclou:

* **Monitoratge constant** dels nodes i serveis.
* **MigraciÃ³ o reinici automÃ tic** de mÃ quines virtuals en cas de fallida.
* **Policies de gestiÃ³ de recursos**, com assignaciÃ³ preferida de nodes o prioritats.
* **IntegraciÃ³ amb lâ€™emmagatzematge compartit (ex. Ceph)** per garantir que les dades estiguen disponibles des de qualsevol node.

#### ğŸ§© Avantatges clau:

* **ContinuÃ¯tat del servei** sense intervencions manuals.
* **Millora de la tolerÃ ncia a fallades** en entorns crÃ­tics.
* **ReducciÃ³ de riscos de pÃ¨rdua de dades** grÃ cies a la integraciÃ³ amb sistemes com Ceph i PBS.
* **Augment de la confianÃ§a operativa**, especialment en serveis que han dâ€™estar actius 24/7.

En resum, la **Alta Disponibilitat** Ã©s un component fonamental en infraestructures professionals, ja que **automatitza la resposta davant incidÃ¨ncies**, mantÃ© els serveis actius i contribueix a una experiÃ¨ncia dâ€™usuari contÃ­nua i fiable, fins i tot en condicions adverses.

---

# 6. ğŸ’¾ Proxmox Backup Server (PBS)

**Proxmox Backup Server (PBS)** Ã©s una soluciÃ³ de cÃ²pia de seguretat **especÃ­ficament dissenyada per a entorns virtualitzats amb Proxmox VE**. Proporciona una plataforma eficient, rÃ pida i segura per realitzar **backups i restauracions** de mÃ quines virtuals (VMs), contenidors (CTs) i fins i tot discos individuals, garantint la **protecciÃ³ i recuperaciÃ³ de dades** davant de fallades o pÃ¨rdua dâ€™informaciÃ³.

#### ğŸ¯ Finalitat de PBS:

PBS sâ€™encarrega de centralitzar totes les cÃ²pies de seguretat dels recursos virtuals del clÃºster, amb funcionalitats com:

* **Backups incrementals**: nomÃ©s es guarden els blocs que han canviat, reduint drÃ sticament el temps i espai necessari.
* **CompressiÃ³ i deduplicaciÃ³**: optimitza lâ€™espai dâ€™emmagatzematge evitant duplicaciÃ³ de dades entre snapshots.
* **Xifratge (opcional)**: garanteix la confidencialitat de les dades tant en repÃ²s com en trÃ nsit.
* **VerificaciÃ³ de consistÃ¨ncia**: comprova automÃ ticament la integritat dels backups emmagatzemats.

#### âš™ï¸ IntegraciÃ³ amb Proxmox VE:

PBS sâ€™integra directament amb **Proxmox VE**, permetent configurar des de la prÃ²pia interfÃ­cie de Proxmox:

* **Jobs de backup programats**, amb horaris i freqÃ¼Ã¨ncies personalitzades.
* **EstratÃ¨gies de retenciÃ³** per controlar quants snapshots es conserven.
* **Restauracions selectives**, incloent fitxers individuals dins de contenidors o VMs.

Les comunicacions entre Proxmox VE i PBS es realitzen a travÃ©s del protocol **Proxmox Backup Protocol**, altament optimitzat per rendiment i seguretat.

#### ğŸ”’ Seguretat i RecuperaciÃ³:

PBS pot situar-se **fora del clÃºster principal** (recomanat), la qual cosa el converteix en una **Ãºltima lÃ­nia de defensa** en cas de fallida total del clÃºster o corrupciÃ³ de dades. Aquesta separaciÃ³ fÃ­sica i lÃ²gica assegura que, fins i tot si els nodes de Proxmox fallen completament, les cÃ²pies de seguretat puguen ser recuperades des dâ€™un sistema aÃ¯llat.

#### ğŸ§© Beneficis principals:

* **AutomatitzaciÃ³ completa de backups i restauracions**.
* **ReducciÃ³ de lâ€™impacte en el rendiment del clÃºster** grÃ cies al backup incremental.
* **Escalabilitat**: un Ãºnic PBS pot gestionar cÃ²pies de seguretat de mÃºltiples clÃºsters.
* **IntegraciÃ³ perfecta** amb la interfÃ­cie de Proxmox VE i amb suport de CLI i API per a automatitzacions.

En definitiva, **Proxmox Backup Server** Ã©s una eina essencial per garantir la **resiliÃ¨ncia i recuperaciÃ³** del sistema virtualitzat, protegint-lo de pÃ¨rdues accidentals, errors humans o fallades greus de maquinari.

---

# ğŸ‘¥ 7. GestiÃ³ dâ€™Usuaris i Pools de Recursos 

La **gestiÃ³ dâ€™usuaris** dins dâ€™un entorn virtualitzat com **Proxmox VE** Ã©s essencial per controlar **qui pot accedir**, **quÃ¨ pot fer** i **sobre quins recursos pot actuar**. Aquesta gestiÃ³ garanteix la **seguretat, organitzaciÃ³ i eficiÃ¨ncia** en la utilitzaciÃ³ del sistema, especialment en entorns compartits, corporatius o amb administraciÃ³ delegada.

---

#### ğŸ¯ Finalitat de la gestiÃ³ dâ€™usuaris:

Lâ€™objectiu principal Ã©s **definir rols i permisos especÃ­fics per a cada usuari o grup dâ€™usuaris**, segons les seues responsabilitats o necessitats. AixÃ² evita lâ€™accÃ©s indegut a recursos crÃ­tics i redueix el risc dâ€™errors humans que podrien afectar el funcionament del clÃºster o les mÃ quines virtuals.

#### âš™ï¸ Funcionalitats clau a Proxmox VE:

* **CreaciÃ³ dâ€™usuaris locals o via integraciÃ³ externa (LDAP/AD):**
  Permet administrar tant usuaris interns com externs mitjanÃ§ant sistemes dâ€™autenticaciÃ³ centralitzada.

* **AssignaciÃ³ de rols i permisos granulars:**
  Proxmox VE ofereix un sistema flexible de permisos basat en rols (`roles`) que determina quines accions pot realitzar un usuari (crear, modificar, esborrar mÃ quines, accedir a la consola, gestionar backups, etc.).

* **DelegaciÃ³ dâ€™administraciÃ³:**
  Ã‰s possible delegar lâ€™administraciÃ³ parcial del sistema a tÃ¨cnics o usuaris avanÃ§ats sense donar-los accÃ©s complet, millorant la seguretat i separaciÃ³ de funcions (*principi de privilegi mÃ­nim*).

* **DefiniciÃ³ de Pools de Recursos:**
  Els *pools* permeten agrupar mÃ quines virtuals, contenidors i recursos assignats a usuaris o equips, facilitant-ne la gestiÃ³ i limitant el seu accÃ©s nomÃ©s a la seua Ã rea de treball.

#### ğŸ” Avantatges de gestionar correctament els usuaris:

* **Millora la seguretat del sistema** evitant accessos no autoritzats o accions destructives.
* **Facilita la traÃ§abilitat** (log dels usuaris i accions realitzades).
* **Permet una administraciÃ³ delegada controlada** en entorns amb mÃºltiples tÃ¨cnics o departaments.
* **Optimitza lâ€™organitzaciÃ³ dels recursos**, assignant responsabilitats clares.

En resum, la gestiÃ³ dâ€™usuaris a Proxmox VE no sols millora la seguretat, sinÃ³ que Ã©s fonamental per estructurar un entorn **multiusuari estable, escalable i eficient**, tant per a entorns educatius, com empresarials o laboratoris de proves.

---

# 8. ğŸ” Seguretat i Bones PrÃ ctiques

### 8.5 MonitoritzaciÃ³ del sistema amb Netdata

**Netdata** Ã©s una eina de monitoritzaciÃ³ en temps real dissenyada per oferir una visiÃ³ molt detallada del rendiment de sistemes, aplicacions, contenidors i dispositius IoT. Ã‰s coneguda per la seva **interfÃ­cie grÃ fica intuÃ¯tiva** i pel seu enfocament en la **visualitzaciÃ³ immediata** de dades de rendiment, amb una latÃ¨ncia molt baixa.

---

### ğŸ” **QuÃ¨ fa Netdata?**

* Recull metadades del sistema (CPU, RAM, disc, xarxa, processos, etc.)
* Monitoritza serveis i aplicacions (MySQL, nginx, docker, etc.)
* Mostra les dades en **temps real (per segon o menys)**
* Pot funcionar com a eina independent o integrat en una arquitectura de monitoritzaciÃ³ mÃ©s gran.

---

## ğŸ”„ Comparativa amb altres solucions similars

A continuaciÃ³ tens una comparaciÃ³ amb tres eines populars de monitoritzaciÃ³:

| CaracterÃ­stica               | **Netdata**              | **Prometheus + Grafana**          | **Zabbix**                       | **Nagios**                  |
| ---------------------------- | ------------------------ | --------------------------------- | -------------------------------- | --------------------------- |
| **Temps real**               | âœ” (milÂ·lisegons)         | âœ˜ (intervals mÃ­nims de 10-15s)    | âœ˜ (intervals configurables)      | âœ˜ (intervals configurables) |
| **InterfÃ­cie grÃ fica**       | âœ” Moderna i interactiva  | âœ” (Grafana)                       | âœ” PerÃ² mÃ©s complexa              | âœ˜ (mÃ©s bÃ sic o plugins)     |
| **Facilitat dâ€™instalÂ·laciÃ³** | âœ” Molt fÃ cil (una lÃ­nia) | âœ˜ Requereix configurar components | âœ˜ Requereix bastant configuraciÃ³ | âœ˜ Pot ser complexa          |
| **Alertes**                  | âœ” BÃ siques integrades    | âœ” Amb Alertmanager                | âœ” Molt completes                 | âœ” Molt completes            |
| **Escalabilitat**            | âœ” Amb Netdata Cloud      | âœ” Alta amb Thanos/Cortex          | âœ” Alta                           | âœ” Mitjana                   |
| **Consum de recursos**       | âœ” Molt lleuger           | âœ˜ Pot ser alt depenent del cas    | âœ˜ Pot consumir bastant           | âœ” Lleuger                   |
| **Extensibilitat**           | âœ˜ Limitada               | âœ” Molt alt                        | âœ” Alt                            | âœ” Alt                       |

---

## âœ… **Avantatges de Netdata**

1. **InstalÂ·laciÃ³ molt senzilla:** una sola lÃ­nia de comandes.
2. **MonitoritzaciÃ³ en temps real real:** actualitzacions per segon o menys.
3. **Poc consum de recursos:** ideal per a sistemes petits o embeguts.
4. **Dades molt detallades:** milers de mÃ¨triques disponibles per defecte.
5. **InterfÃ­cie web interactiva:** grÃ fics clars i navegaciÃ³ fÃ cil.
6. **Suport per a contenidors i microserveis.**

---

## âŒ **Inconvenients de Netdata**

1. **No estÃ  pensat per a emmagatzematge a llarg termini:** retÃ© dades en memÃ²ria per defecte (encara que es pot integrar amb bases de dades de sÃ¨ries temporals).
2. **Alertes bÃ siques:** menys potent que Zabbix o Prometheus+Alertmanager.
3. **Menys integracions corporatives avanÃ§ades.**
4. **Escalabilitat limitada si no sâ€™utilitza Netdata Cloud.**

---

### ğŸ§© En resum:

* **Vols veure dades en temps real de manera fÃ cil i rÃ pida?** ğŸ‘‰ *Netdata Ã©s ideal.*
* **Necessites anÃ lisi a llarg termini, alertes complexes i integraciÃ³ amb sistemes grans?** ğŸ‘‰ *Millor Prometheus + Grafana o Zabbix.*
* **Tens un entorn molt crÃ­tic amb necessitat dâ€™alertes robustes i historial llarg?** ğŸ‘‰ *Zabbix o Nagios sÃ³n mÃ©s adequats.*

Doncs en el cas dels servidors Ã©s millor NetData i per eixe cas m'he quedat en NetData.

---
## 9. ğŸ“Š Monitoratge Centralitzat amb Zabbix

### ğŸ” 9.1 QuÃ¨ Ã©s Zabbix i funcionalitats principals

Zabbix Ã©s una plataforma de monitoratge open source que permet supervisar en temps real el rendiment i l'estat de sistemes, servidors, mÃ quines virtuals, serveis de xarxa i aplicacions. Proporciona alertes configurables, grÃ fiques avanÃ§ades, dashboards personalitzats i recollida dâ€™estadÃ­stiques a llarg termini, tot des dâ€™una interfÃ­cie web centralitzada.

### âœ… 9.2 JustificaciÃ³ de lâ€™elecciÃ³ de Zabbix front altres solucions

Tot i que existeixen altres plataformes com **Nagios**, **Prometheus** o **Netdata**, sâ€™ha escollit Zabbix per les segÃ¼ents raons tÃ¨cniques:

* **Monitoratge integral** (nivell de xarxa, sistema, servei i aplicaciÃ³) en un Ãºnic entorn.
* **Compatibilitat nativa amb Proxmox VE**, grÃ cies a plantilles ja desenvolupades per a la monitoritzaciÃ³ de nodes, mÃ quines virtuals i serveis Ceph.
* Suport per a **alertes proactives i automatitzaciÃ³ de respostes** davant esdeveniments.
* InterfÃ­cie grÃ fica potent amb panells i visualitzacions configurables per a administradors i usuaris.

A diferÃ¨ncia de **Prometheus**, que requereix diversos components externs per una soluciÃ³ completa, o de **Nagios**, que tÃ© un enfocament mÃ©s bÃ sic i menys visual, **Zabbix ofereix una soluciÃ³ tot-en-u** que sâ€™adapta millor a les necessitats del projecte.

### ğŸ”— 9.3 IntegraciÃ³ amb la infraestructura virtualitzada de Proxmox VE

Zabbix es desplegarÃ  com a mÃ quina virtual dins del clÃºster Proxmox, i mitjanÃ§ant lâ€™Ãºs dâ€™agents Zabbix i connexions SNMP, es recollirÃ  informaciÃ³ detallada de lâ€™estat de cada node, VM, recursos de Ceph i altres serveis crÃ¬tics. Sâ€™utilitzaran **templates oficials i personalitzades** per adaptar la monitoritzaciÃ³ als requisits de lâ€™entorn.

### ğŸ›¡ï¸ 9.4 Desplegament en Alta Disponibilitat (HA)

Per garantir la **continuitat del monitoratge fins i tot en cas de fallada dâ€™un node del clÃºster**, el servidor Zabbix estarÃ  definit com a **recurs dâ€™alta disponibilitat (HA)** dins de Proxmox. AixÃ² implica:

* AssignaciÃ³ a un **grup HA**.
* ConfiguraciÃ³ del servei Zabbix com a recurs gestionat per `ha-manager`.
* En cas de caiguda del node actiu, **el servei es migrarÃ  automÃ ticament** a un altre node disponible, assegurant una supervisiÃ³ contÃ­nua.

---

## ğŸ§  10. Conclusions i ValoraciÃ³ Personal

### ğŸ¯ 10.1 Objectius Aconseguits

Al llarg del desenvolupament dâ€™aquest projecte, sâ€™han assolit amb Ã¨xit els objectius plantejats inicialment, tant a nivell tÃ¨cnic com formatiu.

Sâ€™ha aconseguit desplegar una infraestructura virtualitzada completa mitjanÃ§ant **Proxmox VE**, incloent-hi:

* La creaciÃ³ i configuraciÃ³ dâ€™un **clÃºster funcional** amb diversos nodes virtuals.
* La implementaciÃ³ dâ€™un sistema dâ€™**emmagatzematge distribuÃ¯t amb Ceph**, garantint alta disponibilitat de les dades.
* La integraciÃ³ i Ãºs de **Proxmox Backup Server (PBS)** com a sistema de cÃ²pies de seguretat centralitzades.
* La configuraciÃ³ del sistema de **Alta Disponibilitat (HA)**, amb gestiÃ³ automÃ tica de fallades i migraciÃ³ de mÃ quines.
* Lâ€™aplicaciÃ³ de mesures de **seguretat, monitoratge (amb Netdata Cloud)** i bones prÃ ctiques dâ€™administraciÃ³.
* La definiciÃ³ de **rols dâ€™usuari i pools de recursos** per a una gestiÃ³ multiusuari eficient.

A mÃ©s, sâ€™ha documentat detalladament cada fase del projecte, facilitant

### 10.2 Dificultats trobades i solucions

âš ï¸ Problema amb els repositoris de **Proxmox Backup Server**
Una de les principals dificultats trobades ha sigut lâ€™actualitzaciÃ³ dels paquets del sistema, ja que per defecte, Proxmox Backup Server ve configurat amb els repositoris enterprise, els quals requereixen una subscripciÃ³ de pagament.

âœ… ***SoluciÃ³ tÃ¨cnica:*** utilitzar repositoris pÃºblics
Per tal de poder actualitzar i instalÂ·lar paquets sense necessitat de subscripciÃ³, es pot configurar el sistema per a fer Ãºs dels repositoris pÃºblics (no enterprise) de **Proxmox.**

---

#### âš ï¸ Problema amb el almacenament del Ceph:

Durant el procÃ©s de configuraciÃ³ i Ãºs del sistema **Ceph** com a emmagatzematge distribuÃ¯t dins del clÃºster Proxmox VE, es va presentar una **incidÃ¨ncia crÃ­tica relacionada amb la pÃ¨rdua de redundÃ ncia de les dades**.

Concretament, desprÃ©s dâ€™un perÃ­ode de funcionament estable, es va detectar que lâ€™estat general del clÃºster Ceph passava a **WARNING**. Lâ€™anÃ lisi dels logs i lâ€™Ãºs del comandament `ceph status` van indicar que **alguns objectes havien perdut la seua redundÃ ncia**, mostrant missatges com *â€œDegraded data redundancyâ€* o *â€œSome PGs are undersizedâ€*. La causa principal va ser que el **nivell dâ€™ocupaciÃ³ dels discos OSD havia superat el llindar crÃ­tic**, impedint a Ceph replicar adequadament els objectes entre nodes.

Aquest comportament Ã©s esperable en entorns Ceph, ja que per garantir la replicaciÃ³ i integritat de les dades, el sistema necessita un marge suficient de capacitat lliure. Un cop aquest marge desapareix, el sistema prioritza la protecciÃ³ de les dades existents perÃ² ja **no pot garantir la redundÃ ncia completa**, fet que suposa un risc en cas de fallada addicional dâ€™un OSD o node.

#### âœ… SoluciÃ³ adoptada:

Per resoldre aquest problema, es va procedir a:

1. **Avaluar lâ€™ocupaciÃ³ real de cada OSD** mitjanÃ§ant `ceph osd df` per identificar els mÃ©s saturats.
2. **Alliberar espai en el pool Ceph RBD**, eliminant snapshots innecessaris i cÃ²pies de mÃ quines virtuals que no requerien retenciÃ³ prolongada.
3. **Ampliar la capacitat del clÃºster**, afegint un nou OSD amb un disc addicional per restablir el balanÃ§ de dades i la capacitat de replicaciÃ³.
4. Un cop recuperada la capacitat suficient, Ceph va **recomenÃ§ar automÃ ticament el reequilibri (rebalancing)** i va restaurar la redundÃ ncia completa dels objectes afectats.

Aquesta experiÃ¨ncia va posar en relleu la **importÃ ncia de monitorar proactivament lâ€™espai lliure** dins dâ€™un entorn Ceph i definir alertes abans dâ€™arribar a llindars crÃ­tics, aixÃ­ com planificar amb antelaciÃ³ lâ€™escalabilitat del sistema dâ€™emmagatzematge.

---

### ğŸš€ 10.3 Possibles millores futures

#### **1. Docker com a Complement a LXC**  
ğŸ“Œ *Millora la flexibilitat i portabilitat dels contenidors*  
- **Objectiu**: Integrar Docker dins de VMs/containers per aprofitar:  
  - ğŸ‹ Ecosistema mÃ©s ampli d'imatges preconfigurades  
  - ğŸ”„ Compatibilitat amb Kubernetes i eines CI/CD  
  - ğŸ› ï¸ Plantilles predefinides amb Docker + Portainer  
- **Reptes**:  
  - Configurar *systemd* en LXC existents  
  - Establir polÃ­tiques de seguretat especÃ­fiques  

---

#### **2. Seguretat AvanÃ§ada**  
ğŸ” *Hardening del cluster i xifrat de dades*  
- **Certificats TLS personalitzats**:  
  ```bash  
  pvecm updatecerts -force  # Actualitza certificats autofirmats  
  ```  
- **Xifrat de discs amb LUKS** (per a PBS/Ceph):  
  ```bash  
  cryptsetup luksFormat /dev/sdX  # Xifrat en repÃ²s  
  ```  
- **IntegraciÃ³ amb LDAP/AD** per a gestiÃ³ centralitzada dâ€™usuaris.  
- 
---

#### **3. Xarxa i AÃ¯llament**  
ğŸŒ *SegmentaciÃ³ per a major seguretat*  
- **VLANs dedicades**:  
  ```  
  auto vmbr0.100  
  iface vmbr0.100 inet static  
      address 192.168.100.2/24  
      vlan-raw-device vmbr0  
  ```  
  - Separar trÃ nsit de gestiÃ³, Ceph i VMs.  

---

### **ğŸ“‹ Resum de Prioritats**  
| **Ã€rea**          | **AcciÃ³ Clau**                          | **Benefici Principal**                |  
|--------------------|----------------------------------------|---------------------------------------|  
| **Contenidors**    | IntegraciÃ³ Docker + Portainer          | Portabilitat i ecosistema ampliat     |  
| **Seguretat**      | Hardening + LUKS + LDAP                | ProtecciÃ³ de dades i accÃ©s controlat  |  
| **Xarxa**          | VLANs Dedicades                        | SegmentaciÃ³ per a major seguretat     |  

---

### ğŸ¯ 10.4 ValoraciÃ³ tÃ¨cnica i personal del projecte

Aquestes millores convertiran el nostre entorn en un sistema **mÃ©s robust, segur i fÃ cil de gestionar**, adaptant-se tant a entorns educatius com empresarials.  

### ValoraciÃ³ personal del projecte

Aquest projecte mâ€™ha permÃ©s consolidar coneixements adquirits durant el cicle formatiu, especialment en Ã rees com la **virtualitzaciÃ³**, lâ€™**alta disponibilitat** i la **gestiÃ³ dâ€™infraestructures TI**. A travÃ©s de la implementaciÃ³ prÃ ctica amb **Proxmox VE**, he pogut entendre millor el funcionament dels **clÃºsters**, lâ€™**emmagatzematge distribuÃ¯t amb Ceph** i la importÃ ncia de les **cÃ²pies de seguretat mitjanÃ§ant PBS**.

A mÃ©s, la incorporaciÃ³ de **Zabbix com a sistema de monitoratge** ha sigut fonamental per adquirir competÃ¨ncies en la supervisiÃ³ dâ€™entorns TI. La configuraciÃ³ dâ€™agents en sistemes **Windows i Linux**, aixÃ­ com la creaciÃ³ i gestiÃ³ de *hosts* des de la interfÃ­cie web de Zabbix, mâ€™ha ajudat a entendre com controlar el rendiment, detectar anomalies i mantenir la salut de la infraestructura en temps real.

A nivell acadÃ¨mic, ha sigut una experiÃ¨ncia molt completa, ja que mâ€™ha ajudat a connectar la teoria amb la prÃ ctica, millorant la meua **capacitat dâ€™anÃ lisi, resoluciÃ³ de problemes i documentaciÃ³ tÃ¨cnica**. Considere que ha sigut un projecte molt Ãºtil per a preparar-me de cara a **entorns reals i futurs reptes professionals** en el sector de les tecnologies de la informaciÃ³.

---

## ğŸ“ 11. Annexos

### 11.1 Bibliografia

A continuaciÃ³ es detallen les fonts utilitzades per al desenvolupament del projecte:

1. Proxmox. *DocumentaciÃ³ oficial de Proxmox VE*. AccÃ©s 29 dâ€™abril de 2025. [ Proxmox ](https://pve.proxmox.com/wiki/Main_Page).
2. Debian Project. *Debian Wiki*. AccÃ©s 25 dâ€™abril de 2025. [Debian](https://wiki.debian.org/).
3. GitHub. *Repo*. AccÃ©s de seguit.[ Projecte Proxmox ](https://github.com/jcorbii/Projecte_Proxmox/)
4. Netdata  *InstalaciÃ³ Netdata*. AccÃ©s 12 de maig de 2025. [Netdata](https://www.netdata.cloud/)
5. Zabbix  *DocuemntaciÃ³ Zabbix*. AccÃ©s 14 de maig de 2025. [Zabbix](https://www.zabbix.com/)


# Instalacio

## Proxmox

## ğŸ’» 3.  ImplementaciÃ³ del *ClÃºster* Proxmox

### 3.1  InstalÂ·laciÃ³ dels nodes Proxmox VE

#### ğŸ§± InstalÂ·laciÃ³ del primer node de Proxmox

**Passos per a la instalÂ·laciÃ³:**

1. Baixem la imatge *ISO* de Proxmox des de la [web oficial](https://proxmox.com/en/downloads), triant lâ€™Ãºltima versiÃ³ disponible.
2. Una vegada descarregada, la colÂ·loquem en el dispositiu des d'on farem la instalÂ·laciÃ³ en lâ€™equip.

---

ğŸ”¸ El primer pas, desprÃ©s de colÂ·locar la *ISO*, Ã©s la cÃ rrega del menÃº *GRUB*, on hem de seleccionar el procÃ©s dâ€™instalÂ·laciÃ³ desitjat. En este cas, triarem l'opciÃ³ amb interfÃ­cie grÃ fica.


ğŸ”¸ A continuaciÃ³, acceptem la **llicÃ¨ncia dâ€™Ãºs** del programari.

ğŸ”¸ En el segÃ¼ent pas, seleccionem en quin disc volem instalÂ·lar Proxmox. En este exemple nomÃ©s tenim un disc disponible, aixÃ­ que el seleccionem. TambÃ© podem configurar el sistema de fitxers. Triem **ext4**.

<img src="../../../img/image-2.png" alt="GRUB" width="60%">

ğŸ”¸ Assignem la totalitat de lâ€™espai disponible al disc, ja que nomÃ©s n'hi ha un.

<img src="../../../img/image-3.png" alt="GRUB" width="60%">

ğŸ”¸ Configurem la **zona horÃ ria**.


ğŸ”¸ IntroduÃ¯m la **contrasenya dâ€™administraciÃ³** i un **correu electrÃ²nic** per a notificacions del sistema.

ğŸ”¸ Assignem el **nom del *host***, la **IP**, el **gateway** i els **DNS**.

<img src="../../../img/image-6.png" alt="GRUB" width="60%">

ğŸ”¸ Finalment, es mostra un **resum de la configuraciÃ³** triada. Confirmem i iniciem la instalÂ·laciÃ³.

<img src="../../../img/image-7.png" alt="GRUB" width="60%">

ğŸ”¸ Un cop finalitzada la instalÂ·laciÃ³, a la consola apareixerÃ  un missatge indicant que podem accedir a la interfÃ­cie web de Proxmox via:

```
https://10.10.10.60:8006
```

<img src="../../../img/image-8.png" alt="GRUB" width="60%">

ğŸ”¸ AixÃ­ accedim a la **interfÃ­cie web de Proxmox VE**:

<img src="../../../img/image-11.png" alt="GRUB" width="60%">

---

### ğŸ–¥ï¸ InstalÂ·laciÃ³ del Node 2

El procÃ©s dâ€™instalÂ·laciÃ³ del **segon node** Ã©s **idÃ¨ntic** al del primer, excepte pels valors del **nom del host** i la **IP**, que han de ser Ãºnics per a cada node.

<img src="../../../img/image-8.png" alt="GRUB" width="60%">

Com es pot comprovar en el resum, lâ€™Ãºnica diferÃ¨ncia Ã©s la IP i el nom del host.

<img src="../../../img/image-9.png" alt="GRUB" width="60%">

DesprÃ©s de completar la instalÂ·laciÃ³, tornem a tindre accÃ©s a la interfÃ­cie web per la nova IP configurada:

```
https://10.10.10.61:8006
```

<img src="../../../img/image-12.png" alt="GRUB" width="60%">

I amb aixÃ², accedim de nou a la interfÃ­cie de gestiÃ³ de Proxmox:

<img src="../../../img/image-13.png" alt="GRUB" width="60%">

---

### ğŸ–¥ï¸ InstalÂ·laciÃ³ del Node 3

El procÃ©s dâ€™instalÂ·laciÃ³ del **segon node** Ã©s **idÃ¨ntic** al del primer, excepte pels valors del **nom del host** i la **IP**, que han de ser Ãºnics per a cada node.

<img src="../../../img/image-29.png" alt="GRUB" width="60%">

Com es pot comprovar en el resum, lâ€™Ãºnica diferÃ¨ncia Ã©s la IP i el nom del host.

<img src="../../../img/image-30.png" alt="GRUB" width="60%">

DesprÃ©s de completar la instalÂ·laciÃ³, tornem a tindre accÃ©s a la interfÃ­cie web per la nova IP configurada:

```
https://10.10.10.58:8006
```

<img src="../../../img/image-31.png" alt="GRUB" width="60%">

I amb aixÃ², accedim de nou a la interfÃ­cie de gestiÃ³ de Proxmox:

<img src="../../../img/image-32.png" alt="GRUB" width="60%">

## Proxmox Backup Server

## ğŸ’» 6 Proxmox Backup Server (PBS)

### 6.1 InstalaciÃ³n de PBS

**Passos per a la instalÂ·laciÃ³:**

1. Baixem la imatge *ISO* de Proxmox Backup Server des de la [web oficial](https://proxmox.com/en/downloads), triant lâ€™Ãºltima versiÃ³ disponible.
2. Una vegada descarregada, la colÂ·loquem en el dispositiu des d'on farem la instalÂ·laciÃ³ en lâ€™equip.

---

ğŸ”¸ El primer pas, desprÃ©s de colÂ·locar la *ISO*, Ã©s la cÃ rrega del menÃº *GRUB*, on hem de seleccionar el procÃ©s dâ€™instalÂ·laciÃ³ desitjat. En este cas, triarem l'opciÃ³ amb interfÃ­cie grÃ fica.


ğŸ”¸ A continuaciÃ³, acceptem la **llicÃ¨ncia dâ€™Ãºs** del programari.


ğŸ”¸ En el segÃ¼ent pas, seleccionem en quin disc volem instalÂ·lar Proxmox. En este exemple nomÃ©s tenim un disc disponible, aixÃ­ que el seleccionem. TambÃ© podem configurar el sistema de fitxers. Triem **ext4**.

<img src="../../../img/image-16.png" alt="GRUB" width="60%">

ğŸ”¸ IntroduÃ¯m la **contrasenya dâ€™administraciÃ³** i un **correu electrÃ²nic** per a notificacions del sistema.


ğŸ”¸ Assignem el **nom del *host***, la **IP**, el **gateway** i els **DNS**.

<img src="../../../img/image-18.png" alt="GRUB" width="60%">

ğŸ”¸ Finalment, es mostra un **resum de la configuraciÃ³** triada. Confirmem i iniciem la instalÂ·laciÃ³.

<img src="../../../img/image-19.png" alt="GRUB" width="60%">

ğŸ”¸ Un cop finalitzada la instalÂ·laciÃ³, a la consola apareixerÃ  un missatge indicant que podem accedir a la interfÃ­cie web de Proxmox via:

```
https://10.10.10.123:8006
```

<img src="../../../img/image-20.png" alt="GRUB" width="60%">

ğŸ”¸ AixÃ­ accedim a la **interfÃ­cie web de Proxmox VE**:

<img src="../../../img/image-21.png" alt="GRUB" width="60%">

## Zabbix

## 9. ğŸ“Š Monitoratge Centralitzat amb **Zabbix**

### 1. DescÃ rrega de Zabbix

Per comenÃ§ar, accedim a la web oficial de Zabbix: [https://www.zabbix.com](https://www.zabbix.com)

Una vegada dins, cal anar a lâ€™apartat **Download Zabbix**, on seleccionarem:

* La **versiÃ³** desitjada (en aquest cas, la 7.2)
* El **sistema operatiu** (Debian 12)
* Els **components** (servidor, frontend, agent)
* El tipus de **base de dades** (MySQL/MariaDB)
* El servidor web (Apache)

<img src="../../../img/image-127.png" alt="GRUB" width="60%">

---

### 2. InstalÂ·laciÃ³ i configuraciÃ³ del servidor Zabbix

#### a. InstalÂ·laciÃ³ del repositori oficial

```bash
wget https://repo.zabbix.com/zabbix/7.2/release/debian/pool/main/z/zabbix-release/zabbix-release_latest_7.2+debian12_all.deb
dpkg -i zabbix-release_latest_7.2+debian12_all.deb
apt update
```

<img src="../../../img/image-128.png" alt="GRUB" width="60%">

---

#### b. InstalÂ·laciÃ³ dels paquets principals

InstalÂ·lem el servidor Zabbix, el frontend web amb Apache, els scripts SQL i lâ€™agent:

```bash
apt install zabbix-server-mysql zabbix-frontend-php zabbix-apache-conf zabbix-sql-scripts zabbix-agent
```

<img src="../../../img/image-129.png" alt="GRUB" width="60%">

---

#### c. CreaciÃ³ de la base de dades

Asseguraâ€™t que el servidor de bases de dades (MariaDB o MySQL) estÃ  operatiu.

Accedim al client de MySQL per crear la base de dades i lâ€™usuari:

```bash
mysql -uroot -p
```

```sql
CREATE DATABASE zabbix CHARACTER SET utf8mb4 COLLATE utf8mb4_bin;
CREATE USER zabbix@localhost IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON zabbix.* TO zabbix@localhost;
SET GLOBAL log_bin_trust_function_creators = 1;
QUIT;
```

<img src="../../../img/image-130.png" alt="GRUB" width="60%">

---

#### d. ImportaciÃ³ de lâ€™esquema de dades

Des del servidor Zabbix, importem lâ€™esquema i les dades inicials:

```bash
zcat /usr/share/zabbix/sql-scripts/mysql/server.sql.gz | mysql --default-character-set=utf8mb4 -uzabbix -p zabbix
```

<img src="../../../img/image-131.png" alt="GRUB" width="60%">

DesprÃ©s, restaurem el valor per defecte de la directiva `log_bin_trust_function_creators`:

```bash
mysql -uroot -p
```

```sql
SET GLOBAL log_bin_trust_function_creators = 0;
QUIT;
```

<img src="../../../img/image-132.png" alt="GRUB" width="60%">

---

#### e. ConfiguraciÃ³ del servidor Zabbix

Editem el fitxer de configuraciÃ³ del servidor `/etc/zabbix/zabbix_server.conf` i establim la contrasenya de la base de dades:

```bash
DBPassword=password
```

<img src="../../../img/image-133.png" alt="GRUB" width="60%">

---

#### f. Inici dels serveis

Reiniciem els serveis necessaris i els activem perquÃ¨ sâ€™inicien automÃ ticament amb el sistema:

```bash
systemctl restart zabbix-server zabbix-agent apache2
systemctl enable zabbix-server zabbix-agent apache2
```

<img src="../../../img/image-134.png" alt="GRUB" width="60%">

---

#### g. AccÃ©s a la interfÃ­cie web

Una vegada tot estiga operatiu, podem accedir a la interfÃ­cie web de Zabbix des del navegador:

```
http://IP_DEL_SERVIDOR/zabbix
```

Des dâ€™acÃ­ podrem finalitzar la configuraciÃ³ via web GUI.

<img src="../../../img/image-135.png" alt="GRUB" width="60%">

---

Amb aixÃ², el servidor Zabbix queda instalÂ·lat i llest per a ser utilitzat per a la monitoritzaciÃ³ centralitzada de la infraestructura.

<img src="../../../img/image-137.png" alt="GRUB" width="60%">


# ConfiguraciÃ³ 

# Proxmox

# 3. ğŸ–¥ï¸ ImplementaciÃ³ del ClÃºster Proxmox

A continuaciÃ³ et detallem pas a pas com crear un clÃºster en Proxmox i unir-hi altres nodes.

---

## ğŸ› ï¸ 3.2 ConfiguraciÃ³ del clÃºster (pvecm)

1. Accedeix a un dels nodes de Proxmox.
2. Ves a **Datacenter â†’ Cluster** des del menÃº lateral esquerre.
3. Fes clic a **Crear ClÃºster** (`Create Cluster`).

<img src="../../../img/image-56.png" alt="GRUB" width="60%">

4. Ompli les dades del clÃºster:

   * **Nom del ClÃºster**
   * **InterfÃ­cie de xarxa**
   * Altres parÃ metres segons la teua configuraciÃ³


<img src="../../../img/image-57.png" alt="GRUB" width="60%">


<img src="../../../img/image-58.png" alt="GRUB" width="60%">


1. Un cop creat, veurÃ s el node com a part del clÃºster.

<img src="../../../img/image-59.png" alt="GRUB" width="60%">

---

## ğŸ”— 2. Unir Nodes al ClÃºster

Per afegir un altre node al clÃºster:

1. Accedeix al segon node i ves a **Datacenter â†’ Cluster**.
2. Fes clic a **Unir-se al clÃºster** (`Join Cluster`).

<img src="../../../img/image-60.png" alt="GRUB" width="60%">

3. A continuaciÃ³, haurÃ s dâ€™introduir la **informaciÃ³ del clÃºster**.

<img src="../../../img/image-61.png" alt="GRUB" width="60%">


4. Per obtindre aquesta informaciÃ³, torna al node principal del clÃºster i fes clic a **Join Information**.

<img src="../../../img/image-62.png" alt="GRUB" width="60%">

5. Copia aquesta informaciÃ³ i torna al node secundari. Enganxa-la al formulari per unir-se.

<img src="../../../img/image-63.png" alt="GRUB" width="60%">


1. Fes clic a **Unir-se**. Si tot Ã©s correcte, el node sâ€™afegirÃ  automÃ ticament al clÃºster.

<img src="../../../img/image-64.png" alt="GRUB" width="60%">

---

## â• 3. Afegir mÃ©s nodes

Per afegir mÃ©s nodes, repeteix exactament el mateix procÃ©s:

* Accedeix al node
* Ves a **Datacenter â†’ Cluster**
* Fes clic a **Unir-se al clÃºster**
* Copia la informaciÃ³ del node principal
* Enganxa-la i uneix el node

---

ğŸ”š I amb aixÃ² ja tindrÃ s un clÃºster Proxmox funcional amb diversos nodes!

<img src="../../../img/image-64.png" alt="GRUB" width="60%">

Perfecte! Comencem pel punt **4.1 IntroducciÃ³ a Ceph i integraciÃ³ amb Proxmox**. Et deixe a continuaciÃ³ una proposta redactada en valenciÃ  formal, clara i adequada per al teu projecte:

---

### ğŸ§  4 IntroducciÃ³ a **Ceph** i IntegraciÃ³ amb **Proxmox VE**

### 4.2 âš™ï¸ InstalÂ·laciÃ³ i ConfiguraciÃ³ de **Ceph** al ClÃºster

La instalÂ·laciÃ³ de Ceph en un entorn **Proxmox VE** es pot fer de manera centralitzada i senzilla grÃ cies a la seua integraciÃ³ nativa. A continuaciÃ³ es detallen els passos principals per a desplegar Ceph en un clÃºster de Proxmox:

---

#### ğŸ§© Requisits previs

Abans de comenÃ§ar amb la instalÂ·laciÃ³, cal assegurar:

* Tots els nodes tenen lâ€™hora sincronitzada via **NTP**
* Una xarxa especÃ­fica o VLAN dedicada per al trÃ nsit de Ceph (preferiblement amb baixa latÃ¨ncia i alta amplada de banda)
* Discos dedicats per a Ceph (no utilitzar el mateix disc que el sistema operatiu)
* Una configuraciÃ³ bÃ sica del clÃºster de Proxmox ja establida

---

#### ğŸ› ï¸ Passos dâ€™instalÂ·laciÃ³

1. **Accedir a la interfÃ­cie web de Proxmox**

   * Ves a `Datacenter â†’ Ceph`

2. **InstalÂ·lar Ceph als nodes**

   * Selecciona cada node on vols desplegar Ceph
   * A lâ€™apartat `Ceph`, fes clic a **Install Ceph**
   * El sistema instalÂ·larÃ  automÃ ticament els paquets necessaris (`ceph`, `ceph-common`, etc.)

<img src="../../../img/image-66.png" alt="GRUB" width="60%">

<img src="../../../img/image-67.png" alt="GRUB" width="60%">

3. **Crear els monitors (MON)**

   * Un mÃ­nim de **tres monitors** Ã©s recomanat per garantir el quorum
   * Des de lâ€™apartat `Monitor`, fes clic a **Create Monitor**

<img src="../../../img/image-68.png" alt="GRUB" width="60%">

<img src="../../../img/image-69.png" alt="GRUB" width="60%">

4. **Afegir el gestor (MGR)**

   * Necessari per a la interfÃ­cie grÃ fica i gestiÃ³ avanÃ§ada
   * Creaâ€™l des de la mateixa pestanya amb el botÃ³ **Create Manager**

<img src="../../../img/image-70.png" alt="GRUB" width="60%">

5. **Afegir els OSDs (Object Storage Daemons)**

   * Els OSDs sÃ³n els processos que gestionen els discos durs del clÃºster
   * Ves a `OSD â†’ Create OSD`, selecciona el disc fÃ­sic i creaâ€™l
   * Repeteix el procÃ©s per a cada node i disc dedicat

<img src="../../../img/image-71.png" alt="GRUB" width="60%">


<p align="center">
<img src="../../../img/image-72.png" alt="GRUB" width="60%">
</p>

* Com tenim 2 discos per cada node (menos en el node 3 que sols hi ha 1)de proxmox haurem de repetir el proccess dos voltes

**Node 1:**

<img src="../../../img/image-73.png" alt="GRUB" width="60%">

**Node 2**

<img src="../../../img/image-74.png" alt="GRUB" width="60%">

**Node 3**

<img src="../../../img/image-75.png" alt="GRUB" width="60%">

1. **(Opcional) Crear un MDS (Metadata Server)**

   * NomÃ©s si vols utilitzar **CephFS** com a sistema de fitxers compartit

    ##### **ğŸ“‚ QuÃ¨ sÃ³n els metadades?**
    Els metadades sÃ³n informaciÃ³ sobre els fitxers, com ara:

    Noms de fitxers i directoris

    Jerarquia de carpetes

    Permisos dâ€™accÃ©s

    Propietaris

    Dates de creaciÃ³ o modificaciÃ³

    
    ##### ğŸ§  QuÃ¨ fa exactament el MDS?
    
    Quan utilitzes CephFS (el sistema de fitxers distribuÃ¯t de Ceph), el Metadata Server:

    Controla lâ€™estructura i organitzaciÃ³ del sistema de fitxers

    Processa operacions com ls, mkdir, rm, mv, etc.

    Fa que les consultes de fitxers siguen rÃ pides i escalables

    Allibera als OSDs dâ€™aquesta tasca perquÃ¨ es centren nomÃ©s en llegir i escriure dades

Perfecte! AcÃ­ tens el punt **4.3 CreaciÃ³ de pools dâ€™emmagatzematge** redactat de manera formal i clara en valenciÃ , seguint lâ€™estil dels punts anteriors:

---

### 4.3 ğŸ—ï¸ CreaciÃ³ de Pools dâ€™Emmagatzematge en Ceph

Els **pools dâ€™emmagatzematge** sÃ³n una part fonamental en lâ€™arquitectura de **Ceph**, ja que representen els espais lÃ²gics on es distribueixen les dades entre els diferents OSDs del clÃºster. A cada pool se li pot assignar una funciÃ³ especÃ­fica, com ara allotjar mÃ quines virtuals, contenidors o fitxers de CephFS.

---

### ğŸ” QuÃ¨ Ã©s un Pool?

Un **pool** Ã©s una agrupaciÃ³ lÃ²gica dâ€™objectes dins del clÃºster Ceph. Cada objecte dins dâ€™un pool es reparteix entre els OSDs segons una polÃ­tica de distribuciÃ³ definida, garantint aixÃ­ la replicaciÃ³ i la tolerÃ ncia a fallades.

---

### ğŸ› ï¸ CreaciÃ³ dâ€™un Pool pas a pas en Proxmox VE

1. Accedeix a la interfÃ­cie web de **Proxmox VE**
2. Ves a `Datacenter â†’ Ceph â†’ Pools`
3. Fes clic a **Create**

<img src="../../../img/image-76.png" alt="GRUB" width="60%">

4. Emplena els camps segÃ¼ents:



   * **Nom del pool:** (ex. `vm_data`, `cephfs_data`, `backups`)
   * **Nombre de rÃ¨pliques (Size):** recomanat mÃ­nim **3** per a alta disponibilitat
   * **Min. rÃ¨pliques (Min. Size):** mÃ­nim **2** per a mantenir el servei actiu amb una fallada
   * **Crush Rule:** regla de distribuciÃ³ entre els dispositius de disc

<p align="center">
<img src="../../../img/image-77.png" alt="GRUB" width="60%">
</p>

1. Fes clic a **Create** i espera a que el pool aparega a la llista

<img src="../../../img/image-78.png" alt="GRUB" width="60%">

Al pas d'un temps podem veure com en els nodes apareix l'almacenament del ceph.

<img src="../../../img/image-79.png" alt="GRUB" width="60%">

---

### ğŸ§  Consideracions importants

* Els pools amb mÃ©s rÃ¨pliques consumeixen mÃ©s espai perÃ² ofereixen mÃ©s redundÃ ncia.
* Ã‰s possible crear **pools separats** per a diferents usos (ex: un per a VM i un altre per a CephFS).
* Es poden utilitzar **regles CRUSH** per controlar com es distribueixen les dades per racks, discos o ubicacions fÃ­siques.

---

### âœ… Resultat

Amb el pool creat, ja pots:

* Assignar-lo com a **emmagatzematge de mÃ quines virtuals (RBD)**
* Utilitzar-lo per a **CephFS**
* Monitorar el seu estat i Ãºs des de la pestanya de **Ceph â†’ Pools**

Perfecte! A continuaciÃ³ et presente el punt **4.4 Proves de rendiment i replicaciÃ³** redactat de manera formal i clara, mantenint lâ€™estil del teu projecte en valenciÃ :

---

### 4.4 ğŸš€ Proves de Rendiment i ReplicaciÃ³ en Ceph

Una vegada el clÃºster Ceph estÃ  desplegat i operatiu, Ã©s fonamental realitzar proves de **rendiment** i **replicaciÃ³** per a verificar el correcte funcionament de lâ€™emmagatzematge distribuÃ¯t, aixÃ­ com garantir la **fiabilitat** i **eficiÃ¨ncia** del sistema.

---

### ğŸ“Š Proves de rendiment

Les proves de rendiment ens permeten mesurar la **velocitat de lectura i escriptura** dels dispositius Ceph, aixÃ­ com la **latÃ¨ncia** i **capacitat de resposta** del clÃºster.

#### ğŸ§ª Eines recomanades:

* `rados bench` â†’ eina prÃ²pia de Ceph per mesurar el rendiment de lectura/escriptura
* `fio` â†’ eina externa per fer proves personalitzades dâ€™I/O

#### Exemple amb `rados bench`:

```bash
# Prova d'escriptura durant 60 segons
rados bench -p vm-data 60 write --no-cleanup

# Prova de lectura seqÃ¼encial
rados bench -p vm-data 60 seq

# Prova de lectura aleatÃ²ria
rados bench -p vm-data 60 rand
```

<p align="center">
<img src="../../../img/image-80.png" alt="GRUB" width="60%">
</p>

### âœ… Resultat esperat

* Les proves dâ€™escriptura i lectura han de mostrar valors de rendiment estables i adequats segons el teu hardware.
* La replicaciÃ³ ha de funcionar de manera automÃ tica, garantint la integritat i disponibilitat de les dades davant qualsevol fallada.

#### ğŸ“Œ Exemple de comandes via CLI

Alternativament, es pot fer la instalÂ·laciÃ³ via lÃ­nia de comandes:

```bash
# InstalÂ·lar Ceph
pveceph install

# Crear un monitor
pveceph mon create

# Crear un OSD
pveceph osd create /dev/sdX
```

---

### âœ… Resultat

Una vegada configurats els **MON**, **MGR** i **OSD**, el clÃºster Ceph estarÃ  operatiu. Ja pots procedir a:

* Crear **pools dâ€™emmagatzematge**
* Assignar-los com a backend per a mÃ quines virtuals
* Monitorar lâ€™estat del clÃºster des de la interfÃ­cie de Proxmox

---

### â™»ï¸ Proves de replicaciÃ³

Ceph replica les dades entre OSDs segons la configuraciÃ³ de rÃ¨pliques (per defecte 3). Ã‰s important verificar que:

1. **Les dades es repliquen correctament** a mÃºltiples discos.
2. **Quan cau un OSD o node**, Ceph automÃ ticament redistribueix les rÃ¨pliques.

#### ğŸ”„ Prova de fallada simulada:

1. Apaga un OSD manualment:

   ```bash
   systemctl stop ceph-osd@X
   ```

   (on `X` Ã©s el nÃºmero de lâ€™OSD)


2. Observa com Ceph reporta lâ€™estat *degraded* i com reubica les dades.

<img src="../../../img/image-81.png" alt="GRUB" width="60%">

3. Torna a engegar lâ€™OSD i comprova la **reestructuraciÃ³ automÃ tica**:

   ```bash
   systemctl start ceph-osd@X
   ```

<img src="../../../img/image-82.png" alt="GRUB" width="60%">

---

### ğŸ“ˆ 4.5 GestiÃ³ i Monitoratge de **Ceph**

Una vegada desplegat el clÃºster **Ceph**, Ã©s fonamental realitzar una gestiÃ³ i monitoratge continuat per garantir lâ€™estabilitat, el rendiment i la disponibilitat de les dades. Proxmox VE ofereix una **integraciÃ³ nativa amb Ceph**, que facilita tant el control operatiu com la detecciÃ³ anticipada de possibles incidÃ¨ncies.

---

### ğŸ› ï¸ Eines de gestiÃ³ disponibles

Proxmox proporciona diversos mÃ¨todes per gestionar Ceph:

#### ğŸ“Œ InterfÃ­cie web de Proxmox VE

Des de la GUI es pot accedir a:

* **Estat del clÃºster**: `Datacenter â†’ Ceph â†’ Status`
* **GestiÃ³ dâ€™OSDs**: afegir, eliminar o veure lâ€™estat dels Object Storage Daemons
* **Monitors (MONs)** i **Managers (MGRs)**: estat, creaciÃ³ i falles
* **CreaciÃ³ i eliminaciÃ³ de pools**
* **ConfiguraciÃ³ i gestiÃ³ de CephFS**

#### ğŸ”§ LÃ­nia de comandes (CLI)

Per a gestiÃ³ avanÃ§ada i automatitzacions:

```bash
ceph status             # Estat general del clÃºster
ceph osd tree           # VisualitzaciÃ³ jerÃ rquica dels OSDs
ceph df                 # Ãšs dâ€™espai en pools
ceph health detail      # InformaciÃ³ detallada de salut
ceph osd out/in <id>    # Marcar un OSD com a fora o dins del clÃºster
```

---

### ğŸ“Š Monitoratge actiu del clÃºster

Els principals parÃ metres a controlar de forma contÃ­nua sÃ³n:

* **Salut del clÃºster (HEALTH\_OK / WARN / ERR)**
* **Nombre dâ€™OSDs actius i en lÃ­nia**
* **Estat dels MON i MGR**
* **Ãšs de lâ€™espai per pool i per OSD**
* **LatÃ¨ncies de lectura i escriptura**
* **Rebalanceig de dades en cas de fallada o afegit de nous discos**

---

### ğŸ“¡ IntegraciÃ³ amb eines externes

Encara que Proxmox proporciona visualitzaciÃ³ bÃ sica, pots integrar Ceph amb eines de monitoratge mÃ©s potents com:

* **Prometheus + Grafana** (via Ceph MGR modules)
* **Zabbix** o **Nagios**, mitjanÃ§ant plugins
* Alertes per correu electrÃ²nic o sistemes de notificaciÃ³

---

### ğŸ’¡ Recomanacions de manteniment

* ğŸ” **Revisar lâ€™estat del clÃºster regularment**
* ğŸ§ª **Simular fallades controlades** per validar la replicaciÃ³ i recuperaciÃ³
* ğŸ“¦ **No sobrecarregar els OSDs**; mantindre un marge de capacitat lliure
* ğŸ›‘ **Evitar la pÃ¨rdua simultÃ nia de mÃºltiples discos** amb rÃ¨pliques mÃ­nimes

---

### âœ… Resultat

Amb una correcta gestiÃ³ i monitoratge, es garanteix que el clÃºster Ceph oferisca un rendiment estable, altament disponible i resistent a fallades, adaptat a les necessitats de lâ€™entorn virtualitzat en Proxmox VE.


Perfecte! AcÃ­ tens el punt **5. Alta Disponibilitat (HA)** completament redactat en valenciÃ  formal i tÃ¨cnic, amb icones i una estructura clara per a incloureâ€™l al teu projecte:

---

## ğŸ›¡ï¸ 5. Alta Disponibilitat (**HA**) en Proxmox VE

La **Alta Disponibilitat (High Availability)** Ã©s una funcionalitat clau en entorns crÃ­tics, ja que permet garantir la **continuÃ¯tat del servei** davant la caiguda dâ€™un node. Amb Proxmox VE, aquesta caracterÃ­stica sâ€™integra de manera nativa quan es treballa amb un **clÃºster**.

---

### âš™ï¸ 5.1 ActivaciÃ³ del Gestor HA en Proxmox

Per a fer Ãºs de la funcionalitat HA, cal que:

1. El clÃºster estiga format per almenys **3 nodes** (per mantenir el **quorum**)
2. Els nodes tinguen **Corosync** i **pve-ha-crm** actius
3. Els recursos (VM o CT) estiguen ubicats en **storage compartit** o Ceph

#### ğŸ”„ Procediment:

* Ves a `Datacenter â†’ HA`
* Asseguraâ€™t que el **HA Manager** estÃ  actiu
* Cada node mostrarÃ  el seu estat (online, standby, etc.)

<p align="center">
<img src="../../../img/image-83.png" alt="GRUB" width="60%">
</p>

---

### ğŸ§© 5.2 DefiniciÃ³ de Grups HA

Els **grups HA** permeten organitzar i assignar mÃ quines virtuals o contenidors per a gestionar millor les polÃ­tiques de tolerÃ ncia a fallades.

#### CreaciÃ³ dâ€™un grup HA:

1. Ves a `Datacenter â†’ HA â†’ Groups`

<img src="../../../img/image-84.png" alt="GRUB" width="60%">

1. Fes clic a **Create**
2. Assigna:

<p align="center">
<img src="../../../img/image-85.png" alt="GRUB" width="60%">
</p>

   * **Nom del grup**
   * **Nodes preferits** per executar el servei
   * **Prioritats** (per decidir on sâ€™hauria de migrar en cas de fallada)

DesprÃ©s, quan crees o edites una VM/CT, pots assignar-la a un grup HA.

---

### ğŸ” 5.3 Proves de TolerÃ ncia a Fallades (Failover de MÃ quines Virtuals)

Per assegurar el correcte funcionament de la configuraciÃ³ HA, Ã©s recomanable fer proves de **failover controlades**:

#### Escenari de prova:

1. Assigna una VM a un grup HA

<p align="center">
<img src="../../../img/image-86.png" alt="GRUB" width="60%">
</p>

2. Para o apaga un node manualment

<img src="../../../img/image-87.png" alt="GRUB" width="60%">

3. Observa com la VM Ã©s **migrada automÃ ticament** a un altre node disponible
4. Verifica que el servei continua operatiu sense intervenciÃ³ manual

<img src="../../../img/image-89.png" alt="GRUB" width="60%">

ğŸ” Es pot monitorar aquest procÃ©s des de `Datacenter â†’ HA â†’ Status`.

<img src="../../../img/image-90.png" alt="GRUB" width="60%">

Per descomptat! AcÃ­ tens el fragment redactat de manera formal i clara, ideal per afegir com a continuaciÃ³ dins del punt 5.4 o com un subapartat prÃ ctic de **recuperaciÃ³ post-fallada**:

---

### ğŸ’¡ 5.4 Casos dâ€™Ãšs i RecuperaciÃ³ davant Caigudes de Nodes

Els entorns amb HA actiu poden recuperar-se de forma automÃ tica en diferents situacions:

* ğŸ”Œ **Fallada de hardware o energia** en un node
* ğŸ§¯ **Actualitzacions crÃ­tiques** que requereixen reinici
* âš™ï¸ **Errors de sistema** o problemes de rendiment greu

En cada cas:

* El sistema HA detecta la fallada
* MigraciÃ³ automÃ tica a nodes disponibles
* RestauraciÃ³ del servei amb **temps mÃ­nim dâ€™interrupciÃ³**

#### Exemple prÃ ctic:

Una mÃ quina virtual crÃ­tica (servidor web, base de dades, etc.) estÃ  configurada amb HA. Si el node cau inesperadament, aquesta VM es reinicia en un altre node en qÃ¼estiÃ³ de segons, garantint la continuÃ¯tat del servei.

---

### âœ… Resultat

Amb la configuraciÃ³ HA en Proxmox VE, es millora significativament la **resiliÃ¨ncia de la infraestructura virtualitzada**, assegurant que els serveis essencials estiguen disponibles **24/7**, fins i tot davant fallades greus.

---


### ğŸ” RecuperaciÃ³ manual de mÃ quines HA al seu node original

DesprÃ©s dâ€™una **caiguda temporal dâ€™un node** del clÃºster, el sistema **HA de Proxmox** trasllada automÃ ticament les mÃ quines virtuals o contenidors afectats a un altre node disponible per garantir la continuÃ¯tat del servei.

Un cop el node original torna a estar **en lÃ­nia i estable**, Ã©s **recomanable migrar manualment** les mÃ quines al seu node d'origen per:

* Recuperar lâ€™equilibri de cÃ rrega del clÃºster
* Retornar els recursos als seus entorns habituals
* Preparar el sistema per a futures fallades

---

### âš™ï¸ Procediment per a migrar una mÃ quina HA al node original

1. Accedeix a la interfÃ­cie web de Proxmox
2. Ves al node on actualment estÃ  executant-se la mÃ quina
3. Selecciona la mÃ quina virtual o contenidor
4. Fes clic a **"Migrate"**
5. Tria com a destinaciÃ³ el **node original** (ex: `node3`)
6. Confirma lâ€™operaciÃ³

ğŸ“Œ *Nota:* La migraciÃ³ es pot fer en calent (**live migration**) si la mÃ quina suporta aquesta funcionalitat (generalment les VMs amb discs en Ceph o ZFS compartit).

---

### âœ… Resultat

Amb aquest procÃ©s, la mÃ quina recupera la seua ubicaciÃ³ inicial, mantenint-se dins del grup HA i **preparada per a futures gestions automÃ tiques** de tolerÃ ncia a fallades.

<img src="../../../img/image-91.png" alt="GRUB" width="60%">

---

## ğŸ‘¥ 7. GestiÃ³ dâ€™Usuaris i Pools de Recursos

En entorns virtualitzats compartits, com un clÃºster de **Proxmox VE**, Ã©s fonamental establir una **gestiÃ³ dâ€™usuaris estructurada**, amb **permisos diferenciats** i assignaciÃ³ clara de **recursos**, per garantir la **seguretat, control i eficiÃ¨ncia operativa**.

---

### ğŸ” 7.1 CreaciÃ³ de Rols Personalitzats i Permisos

**Proxmox VE** ofereix un sistema de permisos basat en rols, que permet definir quÃ¨ pot fer cada usuari dins del sistema. Aquest model RBAC (Role-Based Access Control) es basa en tres elements:

* **Usuaris** (local, LDAP o via PAM)
* **Rols** (conjunts de permisos)
* **Objectes** (nodes, VM, storage, etc.)

#### ğŸ”§ CreaciÃ³ dâ€™un rol personalitzat:

1. Ves a `Datacenter â†’ Permissions â†’ Roles`
2. Fes clic a **Add**
3. Assigna un nom (ex. `gestor_vm`)
4. Selecciona els permisos especÃ­fics:

   * `VM.Allocate`
   * `VM.Config.Disk`
   * `VM.Console`
   * `Sys.Console`

<img src="../../../img/image-92.png" alt="GRUB" width="60%">

<img src="../../../img/image-93.png" alt="GRUB" width="60%">

#### â• AssignaciÃ³ del rol:

1. Ves a `Permissions â†’ Add â†’ Users`
2. Selecciona:

   * **Path:** Ã rea de control (`/`, `/vms`, `/pool/nom`, etc.)
   * **User:** usuari o grup
   * **Role:** el rol que has creat

AixÃ² permet donar accÃ©s restringit a determinats recursos dins del clÃºster.

<img src="../../../img/image-94.png" alt="GRUB" width="60%">

<img src="../../../img/image-95.png" alt="GRUB" width="60%">

En este cas he creat un usuari de prova per a assignar el rol creat.

<img src="../../../img/image-96.png" alt="GRUB" width="60%">

---

### ğŸ—‚ï¸ 7.2 DefiniciÃ³ de Pools de Recursos

Els **pools** sÃ³n agrupacions lÃ²giques de recursos (VMs, CTs, discos, etc.) que permeten facilitar la gestiÃ³, especialment en entorns multiusuari o amb departaments diferenciats.

#### ğŸ› ï¸ CreaciÃ³ dâ€™un pool:

1. Ves a `Datacenter â†’ Permissions â†’ Pools`
2. Fes clic a **Create**

<img src="../../../img/image-97.png" alt="GRUB" width="60%">

1. Emplena:

   * **Nom del pool:** ex. `departament_it`, `desenvolupament`
   * **DescripciÃ³** (opcional)

<img src="../../../img/image-98.png" alt="GRUB" width="60%">

1. Afegeix les VMs o CTs desitjades al pool

En este cas anem a fer que el usuari proba puga vore la vm 108(Windows10)

<img src="../../../img/image-99.png" alt="GRUB" width="60%">

Assignacio del pool al usuari proba amb el rol  que hem creat.

<img src="../../../img/image-100.png" alt="GRUB" width="60%">

Els pools sÃ³n Ãºtils per:

* Aplicar permisos a grups dâ€™usuari de forma mÃ©s eficient
* Organitzar recursos segons projectes o Ã rees de treball
* Limitar lâ€™accÃ©s nomÃ©s a les mÃ quines assignades

---

### ğŸ‘¤ 7.3 GestiÃ³ Delegada i Multiusuari

Amb els **rols** i **pools**, es pot habilitar un entorn **multiusuari segur**, on cada usuari o equip tinga accÃ©s nomÃ©s als recursos que li pertoquen.

#### Exemple de gestiÃ³ delegada:

* **Usuari:** `anna@pve`
* **Pool assignat:** `marketing_vms`
* **Rol aplicat:** `PVEVMUser` (amb permisos per iniciar/parar/migrar mÃ quines)
* Resultat: Anna nomÃ©s pot gestionar les VMs del pool `marketing_vms`, sense accedir a cap altre recurs del sistema

<img src="../../../img/image-101.png" alt="GRUB" width="60%">

<img src="../../../img/image-102.png" alt="GRUB" width="60%">

<img src="../../../img/image-103.png" alt="GRUB" width="60%">

<img src="../../../img/image-104.png" alt="GRUB" width="60%">

---

### âœ… Beneficis

* ğŸ”’ Major seguretat mitjanÃ§ant la separaciÃ³ de privilegis
* ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Facilitat per delegar la gestiÃ³ a equips tÃ¨cnics o usuaris finals
* ğŸ§© Escalabilitat per a entorns educatius, empresarials o d'hosting

---

### **8.1. Actualitzacions i Pegats de Seguretat**

âœ… **Accions recomanades:**

* **Actualitzar regularment**:

  ```bash
  apt update && apt dist-upgrade
  ```
* Habilitar les **actualitzacions automÃ tiques de seguretat**:

  ```bash
  apt install unattended-upgrades
  dpkg-reconfigure unattended-upgrades
  ```
* Verificar pegats disponibles en Proxmox:

  ```bash
  pveam update
  ```

---

### **8.2. ConfiguraciÃ³ del Tallafoc en Proxmox**

âœ… **Accions recomanades:**

* Activar el **tallafoc integrat** en Proxmox (GUI: `Datacenter > Firewall`).
* Reglas bÃ siques:

  * Permetre nomÃ©s SSH (port 22), accÃ©s web de Proxmox (8006) i Ceph (si sâ€™utilitza) des dâ€™IPs de confianÃ§a.
  * Bloquejar accessos externs a APIs no necessÃ ries.
* Exemple per permetre accÃ©s web nomÃ©s des dâ€™una IP especÃ­fica:

  ```bash
  pve-firewall localnet add -enable 1 -policy in -action ACCEPT -dport 8006 -source 192.168.1.100
  ```

---

### **8.3. CÃ²pies de Seguretat de la ConfiguraciÃ³**

âœ… **Accions recomanades:**

* **Fer cÃ²pia de seguretat de la configuraciÃ³ del clÃºster**:

  ```bash
  tar -czvf /backup/proxmox_config_$(date +%Y-%m-%d).tar.gz /etc/pve/
  ```
* **Automatitzar les cÃ²pies** amb PBS:

  * Programar cÃ²pies diÃ ries/setmanals de VMs/LXCs (GUI: `PBS > Datastore > Backup Jobs`).
  * Utilitzar **retenciÃ³ incremental** (exemple: 7 cÃ²pies diÃ ries + 4 setmanals).

---

### **8.4. Bones PrÃ ctiques dâ€™AdministraciÃ³**

âœ… **Accions recomanades:**

* **Activar lâ€™autenticaciÃ³ en dos passos (2FA)** per a la GUI de Proxmox (GUI: `Datacenter > Permissions > Users`).
* **Restringir l'accÃ©s per SSH**:

  ```bash
  nano /etc/ssh/sshd_config
  ```

  * Afegir: `PermitRootLogin no`, `PasswordAuthentication no` (usar claus SSH).
* **Monitoratge**:

  * Configurar alertes per correu electrÃ²nic (GUI: `Datacenter > Notifications`).
  * Utilitzar `ceph health` i `pveperf` per supervisar el rendiment.

---

### **8.5. Monitoratge Centralitzat amb Netdata Cloud**

**Netdata** Ã©s una eina de monitoratge en temps real, lleugera i de codi obert, que permet visualitzar de forma detallada lâ€™Ãºs de CPU, memÃ²ria, disc, xarxa, processos i molts altres parÃ metres del sistema.

En aquest projecte sâ€™ha optat per utilitzar **Netdata en mode nÃºvol** (*Netdata Cloud*) per garantir:

* ğŸŒ **Accessibilitat des de qualsevol lloc** amb connexiÃ³ a Internet
* â˜ï¸ **Alta disponibilitat** sense necessitat de desplegar servidors de monitoratge propis
* ğŸ“ˆ VisualitzaciÃ³ centralitzada de tots els nodes Proxmox i del PBS en un Ãºnic panell

#### ğŸ› ï¸ InstalÂ·laciÃ³ i connexiÃ³ al nÃºvol:

1. Crear un compte gratuÃ¯t a [https://app.netdata.cloud](https://app.netdata.cloud)
2. En cada node Proxmox:

   * InstalÂ·lar lâ€™agent:

     ```bash
     bash <(curl -Ss https://my-netdata.io/kickstart.sh)
     ```
   * EnllaÃ§ar-lo al teu compte Netdata amb la comanda proporcionada pel portal (normalment amb `netdata-claim.sh`)

DesprÃ©s dâ€™aixÃ², es podrÃ  visualitzar cada node en temps real des del tauler de **Netdata Cloud**, amb alertes, grÃ fics detallats i control unificat del rendiment del clÃºster.

---

### 8.5 MonitoritzaciÃ³ del sistema amb **Netdata**

#### ğŸ§  QuÃ¨ Ã©s Netdata?

**Netdata** Ã©s una plataforma de monitoritzaciÃ³ en temps real que permet supervisar el rendiment i lâ€™estat de sistemes i serveis de manera molt detallada. Ã‰s una eina **lleugera**, de **codi obert** i fÃ cil dâ€™integrar en entorns Linux, incloent **Proxmox VE**.

Proporciona dades sobre:

* Ãšs de CPU, RAM i disc
* TrÃ fic i estat de la xarxa
* EstadÃ­stiques de processos
* Temperatura, serveis actius, ports, etc.

---

### â˜ï¸ UtilitzaciÃ³ de **Netdata Cloud** al projecte

En lloc de desplegar una instÃ ncia de monitoritzaciÃ³ local o en cada node, en aquest projecte sâ€™utilitzarÃ  la **plataforma centralitzada de Netdata Cloud**.

Aquesta estratÃ¨gia es basa en instalÂ·lar Ãºnicament lâ€™**agent de Netdata** a cada node que es vulga monitoritzar, i connectar-lo al panell de control global de Netdata Cloud.

#### âœ… Avantatges de fer servir el nÃºvol:

* ğŸ”’ **Alta disponibilitat:** La plataforma estÃ  disponible 24/7 des de qualsevol lloc
* ğŸŒ **Accessibilitat centralitzada:** Tots els nodes es poden supervisar des dâ€™un Ãºnic panell
* ğŸ“ˆ **VisualitzaciÃ³ interactiva:** GrÃ fics en temps real i alertes integrades
* ğŸ§© **Zero manteniment de servidors de monitoratge locals**
* ğŸ”” Possibilitat de configurar notificacions (Slack, correu, Discord...)

---

### ğŸ› ï¸ Procediment bÃ sic

1. Crear un compte gratuÃ¯t en [https://app.netdata.cloud](https://app.netdata.cloud)
2. En cada node que es vulga monitoritzar:

   * InstalÂ·lar lâ€™agent amb:

     ```bash
      wget -O /tmp/netdata-kickstart.sh https://get.netdata.cloud/kickstart.sh && sh /tmp/netdata-kickstart.sh --nightly-channel --claim-token 2j7CJC_yS3oDQ9DD4eVlLNMV5ecx0WeqwfvNvfOthCcBCkXRLoysr-TKkc5GLM9BzHmlE9Bb36sQghRHfbOsn4rhSEDnd4TmTaabd__6loq4Vceb_o5BitgLI_1gfT4D5pCzx4o --claim-rooms 6ff6ecc7-275c-4404-a4a0-5fac76e79776 --claim-url https://app.netdata.cloud
     ```

      <img src="../../../img/image-120.png" alt="GRUB" width="60%">

   * Connectar lâ€™agent al compte de Netdata Cloud amb la comanda que proporciona el portal (normalment `netdata-claim.sh`)
3. Accedir al panell de **Netdata Cloud** i visualitzar tots els nodes en temps real

<img src="../../../img/image-121.png" alt="GRUB" width="60%">

---

### âœ… Resultat

Amb aquest sistema, es garanteix una **monitoritzaciÃ³ eficaÃ§ i des de qualsevol lloc**, sense haver de desplegar ni mantindre servidors propis per a lâ€™anÃ lisi. Netdata Cloud facilita una supervisiÃ³ **proactiva i Ã gil** del clÃºster Proxmox i del Proxmox Backup Server (PBS).

## ğŸ§ª Casos PrÃ ctics de GestiÃ³ Delegada i Multiusuari en Proxmox VE

### ğŸ“ **Cas 1: Entorn educatiu amb alumnes de prÃ ctiques**

#### Escenari:

Lâ€™institut ha desplegat un clÃºster de Proxmox per a alumnes del cicle de sistemes. Cada alumne ha de gestionar una VM prÃ²pia, perÃ² sense accÃ©s al sistema complet.

#### ConfiguraciÃ³:

* **Usuari:** `alumne01@pve`
* **Pool:** `alumnes`
* **VM assignada:** `vm104` (alumne01-ubuntu24)
* **Rol:** `PVEVMUser`

<img src="../../../img/image-105.png" alt="GRUB" width="60%">

<img src="../../../img/image-106.png" alt="GRUB" width="60%">

<img src="../../../img/image-107.png" alt="GRUB" width="60%">

<img src="../../../img/image-108.png" alt="GRUB" width="60%">

<img src="../../../img/image-109.png" alt="GRUB" width="60%">

#### Resultat:

Lâ€™alumne pot:

* Engegar/parar la seua VM
* Accedir per consola
* No pot crear ni esborrar mÃ quines
* No pot veure cap altra VM

---

### ğŸ¢ **Cas 2: Departament de Desenvolupament en una empresa**

#### Escenari:

Lâ€™equip de desenvolupament necessita accedir a diverses mÃ quines de testing, perÃ² no ha de poder modificar la infraestructura general.

#### ConfiguraciÃ³:

* **Usuaris:** `david@pve`, `jordi@pve`
* **Pool:** `dev_pool`
* **Rols:** `gestor_vm_custom` (creat amb permisos limitats com `VM.Console`, `VM.Start`, `VM.Shutdown`)

<img src="../../../img/image-110.png" alt="GRUB" width="60%">

<img src="../../../img/image-111.png" alt="GRUB" width="60%">

<img src="../../../img/image-112.png" alt="GRUB" width="60%">

<img src="../../../img/image-113.png" alt="GRUB" width="60%">

<img src="../../../img/image-114.png" alt="GRUB" width="60%">

<img src="../../../img/image-115.png" alt="GRUB" width="60%">

#### Resultat:

Els usuaris poden:

* Utilitzar i gestionar les seues VMs
* No poden crear VMs noves ni modificar configuracions globals

---

### ğŸ› ï¸ **Cas 3: TÃ¨cnic amb accÃ©s complet a un node concret**

#### Escenari:

Un tÃ¨cnic extern colÂ·labora en la gestiÃ³ de sistemes, perÃ² nomÃ©s se li vol donar accÃ©s al node `node3`.

#### ConfiguraciÃ³:

* **Usuari:** `tecnic@pve`
* **Ã€rea assignada:** `/nodes/node3`
* **Rol:** `PVEAdmin`

<img src="../../../img/image-116.png" alt="GRUB" width="60%">

<img src="../../../img/image-117.png" alt="GRUB" width="60%">

<img src="../../../img/image-118.png" alt="GRUB" width="60%">

<img src="../../../img/image-119.png" alt="GRUB" width="60%">

#### Resultat:

TÃ© accÃ©s complet nomÃ©s a les mÃ quines i configuraciÃ³ dâ€™eixe node, perÃ² no pot accedir a altres nodes ni al datacenter.

---

### ğŸ§© **Cas 4: Hosting amb gestiÃ³ delegada per client**

#### Escenari:

Una empresa ofereix mÃ quines virtuals com a servei. Cada client gestiona la seua prÃ²pia mÃ quina.

#### ConfiguraciÃ³:

* **Client:** `client_a@pve`
* **Pool:** `client_a_pool`
* **VM assignada:** `vm104`
* **Rol:** `PVEVMUser`

<img src="../../../img/image-122.png" alt="GRUB" width="60%">

<img src="../../../img/image-123.png" alt="GRUB" width="60%">

<img src="../../../img/image-124.png" alt="GRUB" width="60%">

<img src="../../../img/image-125.png" alt="GRUB" width="60%">

<img src="../../../img/image-126.png" alt="GRUB" width="60%">

#### Resultat:

Cada client pot administrar la seua prÃ²pia mÃ quina, sense cap visibilitat sobre altres clients o parts del sistema.

---

### âœ… Conclusions dels casos prÃ ctics

Aquests escenaris mostren com Proxmox permet adaptar-se fÃ cilment a entorns **multiusuari**, amb control granular de permisos i una gestiÃ³ segura i delegada, mantenint la **seguretat**, **eficiÃ¨ncia** i **flexibilitat** del sistema.

---

# Proxmox Backup Server

# 6. ğŸ’¾ Proxmox Backup Server (PBS)

## ğŸ“‚ 6.2 CreaciÃ³ del Datastore en Proxmox Backup Server

### ğŸ›‘ **Requisits previs**

> âš ï¸ Tots els discos seran esborrats. Asseguraâ€™t que **no continguen dades importants** abans de continuar.

---

### ğŸ”§ 1. Verificar instalÂ·laciÃ³ de ZFS

ğŸ”§  Instalar ZFS (si encara no estÃ¡ instalat)

```bash
apt update
apt install zfsutils-linux -y
```

<img src="../../../img/image-22.png" alt="GRUB" width="60%">

AixÃ² confirma que el sistema estÃ  preparat per a treballar amb pools ZFS.

---

### ğŸ—‚ï¸ 2. CreaciÃ³ del ZFS pool

Existien tres opcions principals per a crear el pool ZFS, depenent del nombre de discos disponibles i les necessitats dâ€™emmagatzematge i redundÃ ncia:

* **OpciÃ³ A**: `striped` â€“ mÃ xim espai perÃ² sense cap tolerÃ ncia a fallades.
* **OpciÃ³ B**: `mirror` â€“ redundÃ ncia completa perÃ² requereix un nombre parell de discos.
* **OpciÃ³ C**: `raidz` â€“ una combinaciÃ³ equilibrada entre espai disponible i tolerÃ ncia a falles (similar a RAID 5).

ğŸ‘‰ **AtÃ©s que en aquesta mÃ quina nomÃ©s disposem de tres discos** (`/dev/vda`, `/dev/vdb` i `/dev/vdc`), la millor opciÃ³ des del punt de vista tÃ¨cnic Ã©s **RAIDZ**, ja que ens ofereix una bona capacitat dâ€™emmagatzematge i alhora permet resistir la fallada dâ€™un disc sense perdre les dades.

<img src="../../../img/image-23.png" alt="GRUB" width="60%">

Per crear el pool:

```bash
zpool create backup-pool raidz /dev/vda /dev/vdb /dev/vdc
```

<img src="../../../img/image-24.png" alt="GRUB" width="60%">

---

### âœ… 3. Verificar lâ€™estat del pool

DesprÃ©s de la creaciÃ³, podem comprovar que el pool funciona correctament:

```bash
zpool status
```

<img src="../../../img/image-25.png" alt="GRUB" width="60%">

Hauries de veure un estat **ONLINE** i el pool anomenat `backup-pool`.

---

### ğŸ—ƒï¸ 4. Crear el datastore en PBS

A travÃ©s de la interfÃ­cie web de PBS:

1. Anar a **Datastore > Create**
2. Omplir els camps:

   * **ID**: `zfs-backup`
   * **Path**: `/backup-pool` (punt de muntatge automÃ tic creat per ZFS)
3. Fer clic a **Create**

- CreaciÃ³ del **Datastore**

<img src="../../../img/image-26.png" alt="GRUB" width="60%">

<img src="../../../img/image-27.png" alt="GRUB" width="60%">

---

### ğŸ”„ 5. ComprovaciÃ³ i muntatge automÃ tic

ZFS gestiona automÃ ticament el muntatge del pool. Per comprovar-ho:

```bash
zfs list
```

Resultat esperat:

```
NAME           USED  AVAIL  REFER  MOUNTPOINT
backup-pool    96K   900G    96K   /backup-pool
```

<img src="../../../img/image-28.png" alt="GRUB" width="60%">

A partir dâ€™aquest moment, el sistema ja pot utilitzar el datastore per a cÃ²pies de seguretat, amb les garanties que ofereix ZFS quant a integritat de dades i rendiment.

Perfecte! AcÃ­ tens la redacciÃ³ millorada en **valenciÃ  formal**, amb **iconografia** i amb les **faltes corregides**, mantenint el format Markdown:

---

## ğŸ’¾ 6.3 IntegraciÃ³ amb Proxmox VE

Per integrar el **Proxmox Backup Server (PBS)** com a sistema dâ€™emmagatzematge dins del clÃºster de **Proxmox VE**, cal seguir els passos segÃ¼ents:

---

### ğŸ” 1. Copiar lâ€™**Empremta digital (Fingerprint)** del PBS

Accedeix al **Proxmox Backup Server** i ves a:

ğŸ“ `Dashboard â†’ Show Fingerprint`

Esta empremta Ã©s necessÃ ria per establir una connexiÃ³ segura entre els nodes de Proxmox VE i el servidor PBS.

<img src="../../../img/image-33.png" alt="GRUB" width="60%">

---

### â• 2. Afegir lâ€™Almacenament al ClÃºster de Proxmox

Una vegada copiada lâ€™empremta, accedim a qualsevol node del clÃºster de **Proxmox VE** i seguim els passos segÃ¼ents:

1. Ves a **Datacenter â†’ Storage**
2. Fes clic a **Add** i selecciona lâ€™opciÃ³ **Proxmox Backup Server**

<img src="../../../img/image-34.png" alt="GRUB" width="60%">

---

### ğŸ“ 3. Omplir les Dades de ConnexiÃ³

Ara introduÃ¯m la informaciÃ³ requerida del servidor PBS:

<p align="center">
<img src="../../../img/image-36.png" alt="GRUB" width="60%">
</p>

* **ID:** Nom identificador per a lâ€™almacenament
* **Server:** IP o domini del servidor PBS
* **Username:** Nom dâ€™usuari per connectar-se
* **Password:** Contrasenya corresponent
* **Nodes:** Nodes del clÃºster que tindran accÃ©s a lâ€™almacenament
* **Datastore:** Nom del repositori, per exemple `zfs-backups`
* **Namespace:** Espai de noms (opcional, si sâ€™utilitzen subespais dins el datastore)

---

### âœ… 4. Confirmar i Finalitzar

Una vegada configurat, el sistema validarÃ  les dades i lâ€™almacenament PBS apareixerÃ  com a disponible per a fer cÃ²pies de seguretat o restauracions.

<img src="../../../img/image-35.png" alt="GRUB" width="60%">

---

ğŸ”š **Amb aquests passos, ja tens el teu Proxmox Backup Server integrat dins del clÃºster de Proxmox VE.** AixÃ² et permet gestionar cÃ²pies de seguretat de forma centralitzada, eficient i segura.

Clar! AcÃ­ tens la redacciÃ³ **revisada, tÃ¨cnica i formal** en valenciÃ , amb correccions gramaticals i millor estructuraciÃ³ del contingut. Mantinc les imatges i el format Markdown:

---

## ğŸ’¡ 6.4 ProgramaciÃ³ de CÃ²pies de Seguretat i CreaciÃ³ de MÃ quines Virtuals i Contenidors

En aquest entorn, treballarem tant amb **contenidors (LXC)** com amb **mÃ quines virtuals (KVM)**. Per a gestionar correctament les cÃ²pies de seguretat i automatitzar-les, primer hem de tindre clar com es creen els recursos que es volen protegir.

---

### ğŸ“¦ DescÃ rrega de plantilles per a Contenidors (CT)

Per a poder crear un contenidor, Ã©s necessari **disposar dâ€™un *template*** (plantilla) corresponent al sistema operatiu desitjat.

1. Ves a la secciÃ³ de **Storage** (almacenament)
2. Selecciona lâ€™opciÃ³ **Templates**
3. Tens diverses maneres dâ€™obtindre una plantilla:

   * ğŸ“¤ **Pujar-la manualment** (upload)
   * ğŸ”— **Descarregar-la des dâ€™una URL externa**
   * ğŸ“¥ **Utilitzar les plantilles predefinides** que ofereix Proxmox

ğŸ“Œ En el nostre cas, utilitzarem la tercera opciÃ³: **plantilles predefinides**

<img src="../../../img/image-37.png" alt="GRUB" width="60%">

Per a aquest projecte, descarregarem i utilitzarem plantilles de:

* **Debian**
* **Fedora**

<img src="../../../img/image-38.png" alt="GRUB" width="60%">

---

### ğŸ“ PreparaciÃ³ per a crear una MÃ quina Virtual (VM)

Per a crear una mÃ quina virtual, Ã©s necessari **pujar una ISO** del sistema operatiu al nostre *storage*. Aquesta ISO sâ€™ubica dins de la categoria de **"ISO Images"**.

1. Ves a `Datacenter â†’ Storage`
2. Selecciona el teu emmagatzematge
3. Fes clic a **Upload**
4. Pujar la imatge ISO corresponent (ex. Debian, Ubuntu, Windows...)

<img src="../../../img/image-39.png" alt="GRUB" width="60%">

---

## ğŸ§± CreaciÃ³ dâ€™un Contenidor (CT)

Un cop tenim el *template* descarregat, podem crear un contenidor amb els passos segÃ¼ents:

### ğŸ§­ Pas 1: Inici de la creaciÃ³

1. Fes clic a **Create CT** (Crear CT)

<img src="../../../img/image-40.png" alt="GRUB" width="60%">

---

### ğŸ“ Pas 2: InformaciÃ³ bÃ sica

Introdueix les dades del contenidor:

* **Node:** on es desplegarÃ 
* **CT ID:** identificador Ãºnic
* **Hostname:** nom del sistema
* **Resource Pool:** (opcional) agrupaciÃ³ de recursos
* **Password:** per a lâ€™accÃ©s del root

<img src="../../../img/image-41.png" alt="GRUB" width="60%">

---

### ğŸ“¦ Pas 3: SelecciÃ³ del *Template*

Selecciona la plantilla que has descarregat anteriorment.

<img src="../../../img/image-42.png" alt="GRUB" width="60%">

---

### ğŸ’½ Pas 4: Emmagatzematge

Indica quin **storage** utilitzarÃ  el contenidor.

<img src="../../../img/image-43.png" alt="GRUB" width="60%">

---

### ğŸ§® Pas 5: ConfiguraciÃ³ de recursos

* **CPU:** nombre de nuclis assignats
<img src="../../../img/image-44.png" alt="GRUB" width="60%">

* **RAM:** memÃ²ria en MB
<img src="../../../img/image-45.png" alt="GRUB" width="60%">

---

### ğŸŒ Pas 6: Xarxa

Defineix la configuraciÃ³ de xarxa (bridge, IP, VLAN, etc.)

<img src="../../../img/image-46.png" alt="GRUB" width="60%">

---

### âœ… FinalitzaciÃ³

Un cop completats tots els passos, el contenidor serÃ  creat i apareixerÃ  a la llista de recursos del node.

<img src="../../../img/image-47.png" alt="GRUB" width="60%">

---

## ğŸ–¥ï¸ CreaciÃ³ dâ€™una MÃ quina Virtual (VM)

Els passos per crear una mÃ quina virtual sÃ³n **molt similars** als del contenidor, amb lâ€™Ãºnica diferÃ¨ncia que:

* Es selecciona una **ISO** en lloc dâ€™un *template*
* Es configura un **disc virtual** (en format qcow2, raw o ZFS)
* Es defineixen opcions dâ€™instalÂ·laciÃ³ del sistema operatiu (com si fos una mÃ quina fÃ­sica)

---

ğŸ” Un cop creats els contenidors i les mÃ quines virtuals, ja es poden **programar cÃ²pies de seguretat regulars** mitjanÃ§ant **Proxmox Backup Server (PBS)** o les eines integrades en Proxmox VE.

Perfecte! A continuaciÃ³ tens el punt **6.4 RestauraciÃ³ de mÃ quines virtuals i contenidors** redactat en valenciÃ  formal i tÃ¨cnic, mantenint la coherÃ¨ncia amb lâ€™estructura del teu projecte:

Clar! AcÃ­ tens el punt **6.4 ProgramaciÃ³ de CÃ²pies de Seguretat** (nota: lâ€™has tornat a enumerar com 6.4, perÃ² com que ja sâ€™ha usat aquest nÃºmero, aquest seria realment el **6.5**, a menys que vulgues reorganitzar). Et deixe el contingut redactat tÃ¨cnicament i en valenciÃ  formal:

---

### ğŸ”„  ProgramaciÃ³ de CÃ²pies de Seguretat

Una correcta **estratÃ¨gia de programaciÃ³ de cÃ²pies de seguretat** Ã©s essencial per a garantir la disponibilitat i recuperaciÃ³ de dades en entorns de producciÃ³. Amb **Proxmox VE** i la integraciÃ³ amb **Proxmox Backup Server (PBS)**, es pot automatitzar aquest procÃ©s de forma eficient.

---

### ğŸ—“ï¸ PlanificaciÃ³ de les cÃ²pies

La planificaciÃ³ ha de tindre en compte:

* **FreqÃ¼Ã¨ncia de cÃ²pia:** diÃ ria, setmanal o mensual segons la criticitat del sistema
* **Hora dâ€™execuciÃ³:** preferentment fora de lâ€™horari productiu per minimitzar impacte
* **Recursos inclosos:** contenidors, mÃ quines virtuals o pools de recursos
* **DuraciÃ³ esperada:** basada en la mida i nombre de sistemes a protegir

---

### ğŸ§  Bones prÃ ctiques de planificaciÃ³

* ğŸ§© **Dividir per grups de cÃ rrega:** programar cÃ²pies per pools o per tipus de mÃ quines
* ğŸ• **Evitar solapaments:** distribuint les tasques durant la nit o cap de setmana
* ğŸ§ª **Fer proves de restauraciÃ³ regulars** per validar les cÃ²pies

---

### ğŸ” IntegraciÃ³ amb polÃ­tiques de retenciÃ³

La programaciÃ³ de cÃ²pies de seguretat ha dâ€™anar acompanyada dâ€™una polÃ­tica de **retenciÃ³** que mantinga un nombre raonable de cÃ²pies antigues per evitar saturaciÃ³ de lâ€™emmagatzematge:

* **RetenciÃ³ curta:** 7 cÃ²pies diÃ ries
* **RetenciÃ³ mitjana:** 4 setmanals
* **RetenciÃ³ llarga:** 6 cÃ²pies mensuals

Aquesta polÃ­tica es pot aplicar automÃ ticament des de la configuraciÃ³ del **storage** PBS a `Datacenter â†’ Storage â†’ pbs â†’ Backup Retention `.

<p align="center">
  <img src="../../../img/image-53.png" alt="GRUB" width="60%">
</p>

---

### âš™ï¸ AutomatitzaciÃ³ des de Proxmox VE

Les tasques de cÃ²pia es poden programar fÃ cilment:

1. `Datacenter â†’ Backup â†’ Add`
2. Selecciona:

   * **Storage (PBS)**
   * **Calendari (cron):** ex. `0 21 * * *` per fer-la a les 21:00h cada dia
   * **Mode:** snapshot, suspend o stop
   * **Recursos:** tots, per pool o per ID

<p align="center">
  <img src="../../../img/image-52.png" alt="GRUB" width="60%">
</p>

---

### âœ… Resultat

Amb una programaciÃ³ adequada i una estratÃ¨gia de retenciÃ³ ben definida, s'assegura la **protecciÃ³ contÃ­nua de les dades** i es redueix el risc de pÃ¨rdua d'informaciÃ³ crÃ­tica, mantenint a la vegada l'emmagatzematge net i eficient.

---

### â™»ï¸ 6.5 RestauraciÃ³ de MÃ quines Virtuals i Contenidors

Una de les funcionalitats mÃ©s potents del **Proxmox Backup Server (PBS)** Ã©s la possibilitat de restaurar cÃ²pies de seguretat de manera rÃ pida i fiable, tant de **mÃ quines virtuals (KVM)** com de **contenidors (LXC)**.

---

### ğŸ” Tipus de restauraciÃ³

Proxmox permet dues modalitats principals de restauraciÃ³:

* **RestauraciÃ³ completa:** es crea una nova instÃ ncia basada en la cÃ²pia de seguretat
* **RestauraciÃ³ in situ:** reemplaÃ§a una mÃ quina existent per una cÃ²pia anterior (amb precauciÃ³)

---

### ğŸ› ï¸ Procediment de restauraciÃ³

#### 1. Accedir al la backup

* Entra a la interfÃ­cie web del **Proxmox **
* Ves a `Datacenter â†’ Storage â†’ pbs`
* Selecciona la cÃ²pia de seguretat desitjada

  <img src="../../../img/image-48.png" alt="GRUB" width="60%">

#### 2. LlenÃ§ar la restauraciÃ³

* Fes clic a **Restore**
* Defineix els parÃ metres segÃ¼ents:

  * **Target Node:** node de Proxmox on es restaurarÃ  la mÃ quina
  * **VM ID nou:** (opcional) si vols crear una nova mÃ quina amb un altre ID
  * **Mode de restauraciÃ³:**

    * **Live restore (per a VMs):** restauraciÃ³ mentre la mÃ quina torna a estar en funcionament (recomanat per a entorns amb poca tolerÃ ncia a caigudes)
  * **Storage:** tria lâ€™emmagatzematge de destÃ­

<p align="center">
  <img src="../../../img/image-50.png" alt="GRUB" width="60%">
</p>

#### 3. Confirmar i monitorar

* Fes clic a **Restore**
* Segueix el procÃ©s mitjanÃ§ant el registre de tasques
* Quan acabe, comprova que la mÃ quina funciona correctament

  <img src="../../../img/image-51.png" alt="GRUB" width="60%">

---

### ğŸ’¡ Consells prÃ ctics

* âœ… Fes proves de restauraciÃ³ periÃ²diques per assegurar que les cÃ²pies sÃ³n vÃ lides
* âœ… Assigna nous ID o noms per evitar conflictes amb mÃ quines existents
* âš ï¸ Evita restaurar sobre una mÃ quina activa si no Ã©s estrictament necessari

---

### âœ… Resultat

Amb aquest procediment, pots restaurar mÃ quines virtuals i contenidors des del **PBS** amb alta flexibilitat i mÃ­nima interrupciÃ³ del servei, millorant aixÃ­ la continuÃ¯tat i seguretat de la infraestructura virtualitzada.

Perfecte! AcÃ­ tens el punt **6.6 EstratÃ¨gia de retenciÃ³ i rotaciÃ³ de backups**, redactat de manera formal, tÃ¨cnica i en valenciÃ , seguint la lÃ­nia del teu projecte:

---

### â™»ï¸ 6.6 EstratÃ¨gia de RetenciÃ³ i RotaciÃ³ de Backups

Una gestiÃ³ adequada de la **retenciÃ³ i rotaciÃ³ de cÃ²pies de seguretat** Ã©s fonamental per a garantir un Ãºs eficient de lâ€™emmagatzematge, aixÃ­ com per assegurar la disponibilitat de punts de restauraciÃ³ recents i rellevants.

---

### ğŸ¯ Objectius de la retenciÃ³

* Mantindre cÃ²pies suficients per cobrir escenaris de recuperaciÃ³ (errors recents, corrupciÃ³, ciberatacs...)
* Evitar la saturaciÃ³ del sistema dâ€™emmagatzematge
* Automatitzar lâ€™eliminaciÃ³ de cÃ²pies antigues innecessÃ ries

---

### ğŸ› ï¸ ConfiguraciÃ³ de la polÃ­tica de retenciÃ³

En **Proxmox Backup Server (PBS)** es poden definir regles especÃ­fiques per a cada tasca de backup, o de manera global per a cada **datastore**.

#### ğŸ“ LocalitzaciÃ³:

* `Datacenter â†’ Storage â†’ pbs â†’ Backup Retention

  <img src="../../../img/image-55.png" alt="GRUB" width="60%">

<p align="center">
  <img src="../../../img/image-54.png" alt="GRUB" width="60%">
</p>

#### ğŸ“ ParÃ metres comuns:

| ParÃ metre      | DescripciÃ³                                 | Exemple |
| -------------- | ------------------------------------------ | ------- |
| `keep-last`    | Nombre d'Ãºltimes cÃ²pies que es conservaran | 3       |
| `keep-daily`   | Nombre de cÃ²pies diÃ ries                   | 7       |
| `keep-weekly`  | CÃ²pies setmanals a mantindre               | 4       |
| `keep-monthly` | CÃ²pies mensuals                            | 6       |
| `keep-yearly`  | CÃ²pies anuals (opcional)                   | 1       |

Aquestes polÃ­tiques poden combinar-se per cobrir tant recuperacions recents com arxius histÃ²rics.

---

### ğŸ”„ ProcÃ©s de rotaciÃ³

1. Quan s'executa una nova cÃ²pia de seguretat, **PBS comprova si s'excedeixen els lÃ­mits configurats**
2. Si Ã©s aixÃ­, **pruna (elimina)** les cÃ²pies mÃ©s antigues segons la polÃ­tica definida
3. Aquest procÃ©s Ã©s **automÃ tic** i es registra en els **logs** del sistema

---

### ğŸ’¡ Recomanacions

* ğŸ§® Defineix polÃ­tiques diferents segons la criticitat de cada mÃ quina o servei
* ğŸ—“ï¸ Combina cÃ²pies **diÃ ries** amb cÃ²pies **mensuals de llarg termini**
* ğŸ§ª Revisa periÃ²dicament lâ€™estat dels datastores i els **logs de prunes**

---

### âœ… Resultat

Amb una estratÃ¨gia de retenciÃ³ ben definida, el sistema mantÃ© un equilibri entre **disponibilitat de dades** i **optimitzaciÃ³ de recursos**, evitant tant la pÃ¨rdua dâ€™informaciÃ³ com la sobrecÃ rrega del sistema dâ€™emmagatzematge.

# Zabbix

# 9.6 IncorporaciÃ³ dâ€™un *host* al sistema de monitoratge amb Zabbix

## ğŸ–¥ï¸ 1. Afegir un host Windows

Per integrar un sistema Windows al monitoratge mitjanÃ§ant **Zabbix**, cal seguir els segÃ¼ents passos:

1. Accedir a la pÃ gina oficial de Zabbix i descarregar el **paquet de lâ€™agent Zabbix** corresponent al sistema operatiu:

<img src="../../../img/image-138.png" alt="GRUB" width="60%">

2. Seleccionar:

   * Sistema operatiu (*Windows*)
   * VersiÃ³ del servidor Zabbix
   * Tipus de xifrat (si Ã©s necessari)
   * Format del paquet

<img src="../../../img/image-139.png" alt="GRUB" width="60%">

3. Un cop descarregat lâ€™instalÂ·lador, executar-lo i seguir lâ€™assistent dâ€™instalÂ·laciÃ³:

<img src="../../../img/image-140.png" alt="GRUB" width="60%">
<img src="../../../img/image-141.png" alt="GRUB" width="60%">
<img src="../../../img/image-142.png" alt="GRUB" width="60%">
<img src="../../../img/image-143.png" alt="GRUB" width="60%">

4. Verificar que el **servei de lâ€™agent Zabbix** sâ€™ha iniciat correctament:

<img src="../../../img/image-144.png" alt="GRUB" width="60%">

5. Finalment, accedir a la interfÃ­cie web de Zabbix i crear el nou host:

   * MenÃº: **Monitoring â†’ Hosts â†’ Create Host**

<img src="../../../img/image-145.png" alt="GRUB" width="60%">
<img src="../../../img/image-146.png" alt="GRUB" width="60%">

---

## ğŸ§ 2. Afegir un host Linux

Per monitoritzar un sistema Linux, cal seguir aquests passos:

1. Accedir a la web de Zabbix i seleccionar lâ€™agent corresponent al sistema (en aquest cas, per a **SUSE Linux Enterprise Server - SLES**).

<img src="../../../img/image-147.png" alt="GRUB" width="60%">

2. Seguir les instruccions per instalÂ·lar lâ€™agent:

### a. Afegir el repositori oficial de Zabbix:

```bash
rpm -Uvh --nosignature https://repo.zabbix.com/zabbix/7.2/release/sles/15/noarch/zabbix-release-latest-7.2.sles15.noarch.rpm
zypper --gpg-auto-import-keys refresh 'Zabbix Official Repository'
```

<img src="../../../img/image-148.png" alt="GRUB" width="60%">

### b. InstalÂ·lar el paquet de lâ€™agent:

```bash
zypper in zabbix-agent
```

<img src="../../../img/image-149.png" alt="GRUB" width="60%">

### c. Configurar el fitxer de configuraciÃ³ de lâ€™agent:

Modificar el fitxer `/etc/zabbix/zabbix_agentd.conf` per definir:

* `Server=` IP del servidor Zabbix
* `Hostname=` nom del dispositiu

<img src="../../../img/image-150.png" alt="GRUB" width="60%">
<img src="../../../img/image-151.png" alt="GRUB" width="60%">

### d. Iniciar i habilitar el servei de lâ€™agent:

```bash
systemctl restart zabbix-agent
systemctl enable zabbix-agent
```

<img src="../../../img/image-152.png" alt="GRUB" width="60%">

3. Afegir el nou host des de la interfÃ­cie web del servidor Zabbix:

<img src="../../../img/image-153.png" alt="GRUB" width="60%">

Un cop afegits els sistemes, apareixeran llistats a lâ€™apartat de *Hosts*:

<img src="../../../img/image-154.png" alt="GRUB" width="60%">

---

ğŸ” Amb aquest procÃ©s, tant equips Windows com Linux poden ser incorporats al sistema de monitoratge, permetent la supervisiÃ³ de mÃ¨triques com consum de CPU, Ãºs de memÃ²ria, estat dels serveis i molt mÃ©s, tot centralitzat des del panell de control de Zabbix.